{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9638787b-eb90-44d3-8c50-1427bd34f858",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils.dataloaders.InstanceDataset import InstanceDataset\n",
    "from utils.plot import plot_trace_prediction\n",
    "import torch\n",
    "import math\n",
    "import h5py\n",
    "from torchinfo import summary \n",
    "from nn.eq_transformer import DataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6916fcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_hdf5_file = \"data/instance_samples/Instance_events_counts_10k.hdf5\"\n",
    "event_metadata_file = \"data/instance_samples/metadata_Instance_events_10k.csv\"\n",
    "noise_hdf5_file = \"data/instance_samples/Instance_noise_1k.hdf5\"\n",
    "noise_metadata_file = \"data/instance_samples/metadata_Instance_noise_1k.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9b7d389",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _shuffle_events(event_metadata_file, noise_metadata_file, random_state, output_event_metadata_file, output_noise_metadata_file, balance=False, remove_empty_phase=False):\n",
    "    \n",
    "    \"\"\" \n",
    "    \n",
    "    Split the list of input data into training, validation, and test set.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    event_metadata_file: str\n",
    "        path to event metadata file \n",
    "    \n",
    "    noise_metadata_file: str\n",
    "        path to event metadata file \n",
    "\n",
    "    random_state: int\n",
    "        random state for reproducibility\n",
    "\n",
    "    output_event_metadata_file: bool\n",
    "       Path to the output event metadata file\n",
    "\n",
    "    output_noise_metadata_file: str\n",
    "       Path to the output noise metadata file\n",
    "\n",
    "    balance: bool\n",
    "       Should have equal noise and events \n",
    "\n",
    "    remove_empty_phase: bool\n",
    "       Remove events that do not have both phases\n",
    "              \n",
    "    Returns\n",
    "    -------   \n",
    "    (output_event_metadata_file, output_noise_metadata_file)\n",
    "    \"\"\"       \n",
    "\n",
    "    noise_metadata = pd.read_csv(noise_metadata_file)\n",
    "    event_metadata = pd.read_csv(event_metadata_file, usecols=[\"trace_name\", 'trace_start_time', 'trace_P_arrival_time', 'trace_S_arrival_time', 'trace_P_arrival_sample', 'trace_S_arrival_sample', 'trace_GPD_P_number', 'trace_GPD_S_number', 'trace_EQT_P_number', 'trace_EQT_S_number', 'trace_EQT_number_detections'])\n",
    "\n",
    "    if remove_empty_phase:\n",
    "        event_metadata = event_metadata[event_metadata[\"trace_P_arrival_sample\"].notna() & event_metadata[\"trace_S_arrival_sample\"].notna()]\n",
    "    \n",
    "    event_metadata = event_metadata.sample(frac=1, random_state=random_state) \n",
    "    noise_metadata = noise_metadata.sample(frac=1, random_state=random_state) \n",
    "\n",
    "    if balance:\n",
    "        event_metadata = event_metadata.head(noise_metadata.shape[0])\n",
    "\n",
    "    event_metadata.to_csv(output_event_metadata_file, index=False)\n",
    "    noise_metadata.to_csv(output_noise_metadata_file, index=False)\n",
    "\n",
    "    return output_event_metadata_file, output_noise_metadata_file \n",
    "\n",
    "\n",
    "shuffled_event_metadata_file, shuffled_noise_metadata_file = _shuffle_events(\n",
    "    event_metadata_file=event_metadata_file,\n",
    "    noise_metadata_file=noise_metadata_file,\n",
    "    random_state=9,\n",
    "    output_event_metadata_file='temp/event_metadata.csv',\n",
    "    output_noise_metadata_file='temp/noise_metadata.csv',\n",
    "    balance=True,\n",
    "    remove_empty_phase=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c04581c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_code</th>\n",
       "      <th>trace_start_time</th>\n",
       "      <th>trace_P_arrival_time</th>\n",
       "      <th>trace_S_arrival_time</th>\n",
       "      <th>trace_P_arrival_sample</th>\n",
       "      <th>trace_S_arrival_sample</th>\n",
       "      <th>trace_name</th>\n",
       "      <th>trace_GPD_P_number</th>\n",
       "      <th>trace_GPD_S_number</th>\n",
       "      <th>trace_EQT_number_detections</th>\n",
       "      <th>trace_EQT_P_number</th>\n",
       "      <th>trace_EQT_S_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RM33</td>\n",
       "      <td>2016-12-04T15:34:43.69Z</td>\n",
       "      <td>2016-12-04T15:35:00.48Z</td>\n",
       "      <td>2016-12-04T15:35:07.35Z</td>\n",
       "      <td>1678</td>\n",
       "      <td>2366.0</td>\n",
       "      <td>11030611.IV.RM33..EH</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RM33</td>\n",
       "      <td>2016-12-04T15:34:44.38Z</td>\n",
       "      <td>2016-12-04T15:35:00.48Z</td>\n",
       "      <td>2016-12-04T15:35:07.35Z</td>\n",
       "      <td>1610</td>\n",
       "      <td>2297.0</td>\n",
       "      <td>11030611.IV.RM33..HN</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>T1201</td>\n",
       "      <td>2016-12-04T15:34:35.50Z</td>\n",
       "      <td>2016-12-04T15:34:58.48Z</td>\n",
       "      <td>2016-12-04T15:35:03.73Z</td>\n",
       "      <td>2298</td>\n",
       "      <td>2823.0</td>\n",
       "      <td>11030611.IV.T1201..HN</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>T1202</td>\n",
       "      <td>2016-12-04T15:34:36.75Z</td>\n",
       "      <td>2016-12-04T15:34:57.47Z</td>\n",
       "      <td>2016-12-04T15:35:01.65Z</td>\n",
       "      <td>2072</td>\n",
       "      <td>2490.0</td>\n",
       "      <td>11030611.IV.T1202..EH</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>T1204</td>\n",
       "      <td>2016-12-04T15:34:40.51Z</td>\n",
       "      <td>2016-12-04T15:34:58.23Z</td>\n",
       "      <td>2016-12-04T15:35:03.27Z</td>\n",
       "      <td>1771</td>\n",
       "      <td>2276.0</td>\n",
       "      <td>11030611.IV.T1204..EH</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>T1211</td>\n",
       "      <td>2016-12-04T15:34:23.35Z</td>\n",
       "      <td>2016-12-04T15:35:01.13Z</td>\n",
       "      <td>2016-12-04T15:35:08.18Z</td>\n",
       "      <td>3778</td>\n",
       "      <td>4483.0</td>\n",
       "      <td>11030611.IV.T1211..EH</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>T1211</td>\n",
       "      <td>2016-12-04T15:34:41.24Z</td>\n",
       "      <td>2016-12-04T15:35:01.13Z</td>\n",
       "      <td>2016-12-04T15:35:08.18Z</td>\n",
       "      <td>1989</td>\n",
       "      <td>2694.0</td>\n",
       "      <td>11030611.IV.T1211..HN</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>T1212</td>\n",
       "      <td>2016-12-04T15:34:17.66Z</td>\n",
       "      <td>2016-12-04T15:34:56.60Z</td>\n",
       "      <td>2016-12-04T15:34:59.98Z</td>\n",
       "      <td>3893</td>\n",
       "      <td>4232.0</td>\n",
       "      <td>11030611.IV.T1212..EH</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>T1212</td>\n",
       "      <td>2016-12-04T15:34:30.55Z</td>\n",
       "      <td>2016-12-04T15:34:56.60Z</td>\n",
       "      <td>2016-12-04T15:34:59.98Z</td>\n",
       "      <td>2605</td>\n",
       "      <td>2943.0</td>\n",
       "      <td>11030611.IV.T1212..HN</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>T1213</td>\n",
       "      <td>2016-12-04T15:34:22.49Z</td>\n",
       "      <td>2016-12-04T15:34:56.72Z</td>\n",
       "      <td>2016-12-04T15:35:00.25Z</td>\n",
       "      <td>3422</td>\n",
       "      <td>3775.0</td>\n",
       "      <td>11030611.IV.T1213..EH</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   station_code         trace_start_time     trace_P_arrival_time  \\\n",
       "3          RM33  2016-12-04T15:34:43.69Z  2016-12-04T15:35:00.48Z   \n",
       "4          RM33  2016-12-04T15:34:44.38Z  2016-12-04T15:35:00.48Z   \n",
       "12        T1201  2016-12-04T15:34:35.50Z  2016-12-04T15:34:58.48Z   \n",
       "13        T1202  2016-12-04T15:34:36.75Z  2016-12-04T15:34:57.47Z   \n",
       "14        T1204  2016-12-04T15:34:40.51Z  2016-12-04T15:34:58.23Z   \n",
       "15        T1211  2016-12-04T15:34:23.35Z  2016-12-04T15:35:01.13Z   \n",
       "16        T1211  2016-12-04T15:34:41.24Z  2016-12-04T15:35:01.13Z   \n",
       "17        T1212  2016-12-04T15:34:17.66Z  2016-12-04T15:34:56.60Z   \n",
       "18        T1212  2016-12-04T15:34:30.55Z  2016-12-04T15:34:56.60Z   \n",
       "19        T1213  2016-12-04T15:34:22.49Z  2016-12-04T15:34:56.72Z   \n",
       "\n",
       "       trace_S_arrival_time  trace_P_arrival_sample  trace_S_arrival_sample  \\\n",
       "3   2016-12-04T15:35:07.35Z                    1678                  2366.0   \n",
       "4   2016-12-04T15:35:07.35Z                    1610                  2297.0   \n",
       "12  2016-12-04T15:35:03.73Z                    2298                  2823.0   \n",
       "13  2016-12-04T15:35:01.65Z                    2072                  2490.0   \n",
       "14  2016-12-04T15:35:03.27Z                    1771                  2276.0   \n",
       "15  2016-12-04T15:35:08.18Z                    3778                  4483.0   \n",
       "16  2016-12-04T15:35:08.18Z                    1989                  2694.0   \n",
       "17  2016-12-04T15:34:59.98Z                    3893                  4232.0   \n",
       "18  2016-12-04T15:34:59.98Z                    2605                  2943.0   \n",
       "19  2016-12-04T15:35:00.25Z                    3422                  3775.0   \n",
       "\n",
       "               trace_name  trace_GPD_P_number  trace_GPD_S_number  \\\n",
       "3    11030611.IV.RM33..EH                   3                   7   \n",
       "4    11030611.IV.RM33..HN                   1                   3   \n",
       "12  11030611.IV.T1201..HN                   5                   5   \n",
       "13  11030611.IV.T1202..EH                   2                   3   \n",
       "14  11030611.IV.T1204..EH                   3                   4   \n",
       "15  11030611.IV.T1211..EH                   0                   4   \n",
       "16  11030611.IV.T1211..HN                   0                   3   \n",
       "17  11030611.IV.T1212..EH                   2                   9   \n",
       "18  11030611.IV.T1212..HN                   2                  12   \n",
       "19  11030611.IV.T1213..EH                   2                   4   \n",
       "\n",
       "    trace_EQT_number_detections  trace_EQT_P_number  trace_EQT_S_number  \n",
       "3                           2.0                 2.0                 2.0  \n",
       "4                           NaN                 NaN                 NaN  \n",
       "12                          NaN                 NaN                 NaN  \n",
       "13                          1.0                 1.0                 1.0  \n",
       "14                          3.0                 3.0                 3.0  \n",
       "15                          1.0                 0.0                 1.0  \n",
       "16                          NaN                 NaN                 NaN  \n",
       "17                          1.0                 1.0                 1.0  \n",
       "18                          NaN                 NaN                 NaN  \n",
       "19                          1.0                 1.0                 1.0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_metadata = pd.read_csv(event_metadata_file, usecols=[\"station_code\", \"trace_name\", 'trace_start_time', 'trace_P_arrival_time', 'trace_S_arrival_time', 'trace_P_arrival_sample', 'trace_S_arrival_sample', 'trace_GPD_P_number', 'trace_GPD_S_number', 'trace_EQT_P_number', 'trace_EQT_S_number', 'trace_EQT_number_detections'])\n",
    "\n",
    "event_metadata = event_metadata[event_metadata[\"trace_P_arrival_sample\"].notna() & event_metadata[\"trace_S_arrival_sample\"].notna()]\n",
    "\n",
    "event_metadata.head(10)\n",
    "# event_metadata.info(verbose=True)\n",
    "# event_metadata.describe()\n",
    "# event_metadata.shape\n",
    "\n",
    "# test = event_metadata['trace_S_arrival_sample'] - event_metadata['trace_P_arrival_sample']\n",
    "# test.describe()\n",
    "\n",
    "# print(event_metadata[\"trace_name\"].iloc[4969])\n",
    "# print(event_metadata[\"trace_P_arrival_sample\"][3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6abdceda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_id</th>\n",
       "      <th>station_network_code</th>\n",
       "      <th>station_code</th>\n",
       "      <th>station_location_code</th>\n",
       "      <th>station_channels</th>\n",
       "      <th>station_latitude_deg</th>\n",
       "      <th>station_longitude_deg</th>\n",
       "      <th>station_elevation_m</th>\n",
       "      <th>station_vs_30_mps</th>\n",
       "      <th>station_vs_30_detail</th>\n",
       "      <th>...</th>\n",
       "      <th>trace_Z_upper_quartile_counts</th>\n",
       "      <th>trace_E_spikes</th>\n",
       "      <th>trace_N_spikes</th>\n",
       "      <th>trace_Z_spikes</th>\n",
       "      <th>trace_name</th>\n",
       "      <th>trace_GPD_P_number</th>\n",
       "      <th>trace_GPD_S_number</th>\n",
       "      <th>trace_EQT_number_detections</th>\n",
       "      <th>trace_EQT_P_number</th>\n",
       "      <th>trace_EQT_S_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20131205T210857</td>\n",
       "      <td>IV</td>\n",
       "      <td>ORZI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HN</td>\n",
       "      <td>45.40560</td>\n",
       "      <td>9.93070</td>\n",
       "      <td>83.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>Vs30 extracted from ShakeMap</td>\n",
       "      <td>...</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20131205T210857.IV.ORZI..HN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20070528T142806</td>\n",
       "      <td>IV</td>\n",
       "      <td>BOB</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HH</td>\n",
       "      <td>44.76792</td>\n",
       "      <td>9.44782</td>\n",
       "      <td>910.0</td>\n",
       "      <td>766.0</td>\n",
       "      <td>Vs30 extracted from ShakeMap</td>\n",
       "      <td>...</td>\n",
       "      <td>1542.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20070528T142806.IV.BOB..HH</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20100131T124455</td>\n",
       "      <td>IV</td>\n",
       "      <td>CRAC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EH</td>\n",
       "      <td>40.38140</td>\n",
       "      <td>16.43500</td>\n",
       "      <td>384.0</td>\n",
       "      <td>575.0</td>\n",
       "      <td>Vs30 extracted from ShakeMap</td>\n",
       "      <td>...</td>\n",
       "      <td>539.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20100131T124455.IV.CRAC..EH</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20181206T001526</td>\n",
       "      <td>IV</td>\n",
       "      <td>HPAC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HH</td>\n",
       "      <td>36.70850</td>\n",
       "      <td>15.03720</td>\n",
       "      <td>70.0</td>\n",
       "      <td>584.0</td>\n",
       "      <td>Vs30 extracted from ShakeMap</td>\n",
       "      <td>...</td>\n",
       "      <td>1576.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20181206T001526.IV.HPAC..HH</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20190720T231539</td>\n",
       "      <td>IV</td>\n",
       "      <td>MMGO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HH</td>\n",
       "      <td>37.66195</td>\n",
       "      <td>12.97673</td>\n",
       "      <td>397.0</td>\n",
       "      <td>801.0</td>\n",
       "      <td>Vs30 extracted from ShakeMap</td>\n",
       "      <td>...</td>\n",
       "      <td>36.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20190720T231539.IV.MMGO..HH</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         source_id station_network_code station_code  station_location_code  \\\n",
       "0  20131205T210857                   IV         ORZI                    NaN   \n",
       "1  20070528T142806                   IV          BOB                    NaN   \n",
       "2  20100131T124455                   IV         CRAC                    NaN   \n",
       "3  20181206T001526                   IV         HPAC                    NaN   \n",
       "4  20190720T231539                   IV         MMGO                    NaN   \n",
       "\n",
       "  station_channels  station_latitude_deg  station_longitude_deg  \\\n",
       "0               HN              45.40560                9.93070   \n",
       "1               HH              44.76792                9.44782   \n",
       "2               EH              40.38140               16.43500   \n",
       "3               HH              36.70850               15.03720   \n",
       "4               HH              37.66195               12.97673   \n",
       "\n",
       "   station_elevation_m  station_vs_30_mps          station_vs_30_detail  ...  \\\n",
       "0                 83.0              297.0  Vs30 extracted from ShakeMap  ...   \n",
       "1                910.0              766.0  Vs30 extracted from ShakeMap  ...   \n",
       "2                384.0              575.0  Vs30 extracted from ShakeMap  ...   \n",
       "3                 70.0              584.0  Vs30 extracted from ShakeMap  ...   \n",
       "4                397.0              801.0  Vs30 extracted from ShakeMap  ...   \n",
       "\n",
       "  trace_Z_upper_quartile_counts  trace_E_spikes  trace_N_spikes  \\\n",
       "0                         10.00             0.0             0.0   \n",
       "1                       1542.00             0.0             0.0   \n",
       "2                        539.25             0.0             0.0   \n",
       "3                       1576.00             0.0             0.0   \n",
       "4                         36.00             0.0             0.0   \n",
       "\n",
       "   trace_Z_spikes                   trace_name  trace_GPD_P_number  \\\n",
       "0             0.0  20131205T210857.IV.ORZI..HN                   1   \n",
       "1             0.0   20070528T142806.IV.BOB..HH                   0   \n",
       "2             0.0  20100131T124455.IV.CRAC..EH                   0   \n",
       "3             0.0  20181206T001526.IV.HPAC..HH                   0   \n",
       "4             0.0  20190720T231539.IV.MMGO..HH                   3   \n",
       "\n",
       "   trace_GPD_S_number  trace_EQT_number_detections  trace_EQT_P_number  \\\n",
       "0                   0                          NaN                 NaN   \n",
       "1                   3                          0.0                 0.0   \n",
       "2                   1                          0.0                 0.0   \n",
       "3                   1                          0.0                 0.0   \n",
       "4                   0                          0.0                 0.0   \n",
       "\n",
       "   trace_EQT_S_number  \n",
       "0                 NaN  \n",
       "1                 0.0  \n",
       "2                 0.0  \n",
       "3                 0.0  \n",
       "4                 0.0  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise_metadata = pd.read_csv(noise_metadata_file)\n",
    "\n",
    "noise_metadata.head()\n",
    "\n",
    "# print(noise_metadata[\"trace_name\"].iloc[170])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea730617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size = 7063\n",
      "################## Accessing first event ##################\n",
      "index = 0\n",
      "Detection target = 1\n",
      "input shape = torch.Size([3, 12000])\n",
      "trace name = 11030611.IV.RM33..EH\n",
      "################## Accessing last event ##################\n",
      "index = 6062\n",
      "Detection target = 1\n",
      "input shape = torch.Size([3, 12000])\n",
      "trace name = 11282341.IV.LTRZ..EH\n",
      "################## Accessing first noise ##################\n",
      "index = 6063\n",
      "Detection target = 0\n",
      "input shape = torch.Size([3, 12000])\n",
      "trace name = 20131205T210857.IV.ORZI..HN\n",
      "################## Accessing last noise ##################\n",
      "index = 7062\n",
      "Detection target = 0\n",
      "input shape = torch.Size([3, 12000])\n",
      "trace name = 20190813T130147.IV.CESX..HH\n",
      "################## Out of Index ##################\n",
      "index = 7063\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 44\u001b[0m\n\u001b[1;32m     42\u001b[0m index \u001b[38;5;241m=\u001b[39m noise_metadata\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m event_metadata\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 44\u001b[0m f1, t1, p1, s1, trace_name \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/Documents/personal/school/H2024/IFT6759/earthquake/utils/dataloaders/InstanceDataset.py:67\u001b[0m, in \u001b[0;36mInstanceDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     65\u001b[0m index \u001b[38;5;241m=\u001b[39m (idx \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevents_size) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnoise_start_index\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# print(index)\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m trace_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnoise_metadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrace_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# print(trace_name)\u001b[39;00m\n\u001b[1;32m     69\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnoise_hdf5_file\n",
      "File \u001b[0;32m~/Documents/personal/school/H2024/IFT6759/earthquake/env/lib/python3.11/site-packages/pandas/core/indexing.py:1191\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1189\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[1;32m   1190\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001b[0;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/personal/school/H2024/IFT6759/earthquake/env/lib/python3.11/site-packages/pandas/core/indexing.py:1752\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index by location index with a non-integer key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1751\u001b[0m \u001b[38;5;66;03m# validate the location\u001b[39;00m\n\u001b[0;32m-> 1752\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_integer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1754\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_ixs(key, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[0;32m~/Documents/personal/school/H2024/IFT6759/earthquake/env/lib/python3.11/site-packages/pandas/core/indexing.py:1685\u001b[0m, in \u001b[0;36m_iLocIndexer._validate_integer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1683\u001b[0m len_axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis))\n\u001b[1;32m   1684\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m len_axis \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m-\u001b[39mlen_axis:\n\u001b[0;32m-> 1685\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msingle positional indexer is out-of-bounds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "#Pass split percentage = [1] to not split and access the whole data\n",
    "\n",
    "dataset = InstanceDataset(event_hdf5_file, event_metadata_file, noise_hdf5_file, noise_metadata_file, \"binary\", split_index=0, split_percentage=[1], padding_type=\"sample\", start_padding_value=100, end_padding_value=1000, target_phase=True, phase_padding=20)\n",
    "\n",
    "print(f\"Dataset size = {len(dataset)}\")\n",
    "\n",
    "print(\"################## Accessing first event ##################\")\n",
    "index = 0\n",
    "f1, t1, p1, s1, trace_name = dataset[index]\n",
    "print(f\"index = {index}\")\n",
    "print(f\"Detection target = {t1}\")\n",
    "print(f\"input shape = {f1.shape}\")\n",
    "print(f\"trace name = {trace_name}\")\n",
    "\n",
    "print(\"################## Accessing last event ##################\")\n",
    "index = event_metadata.shape[0] - 1\n",
    "f1, t1, p1, s1, trace_name = dataset[index]\n",
    "print(f\"index = {index}\")\n",
    "print(f\"Detection target = {t1}\")\n",
    "print(f\"input shape = {f1.shape}\")\n",
    "print(f\"trace name = {trace_name}\")\n",
    "\n",
    "\n",
    "print(\"################## Accessing first noise ##################\")\n",
    "index = event_metadata.shape[0]\n",
    "f1, t1, p1, s1, trace_name = dataset[index]\n",
    "print(f\"index = {index}\")\n",
    "print(f\"Detection target = {t1}\")\n",
    "print(f\"input shape = {f1.shape}\")\n",
    "print(f\"trace name = {trace_name}\")\n",
    "\n",
    "\n",
    "print(\"################## Accessing last noise ##################\")\n",
    "index = noise_metadata.shape[0] + event_metadata.shape[0] - 1\n",
    "f1, t1, p1, s1, trace_name = dataset[index]\n",
    "print(f\"index = {index}\")\n",
    "print(f\"Detection target = {t1}\")\n",
    "print(f\"input shape = {f1.shape}\")\n",
    "print(f\"trace name = {trace_name}\")\n",
    "\n",
    "print(\"################## Out of Index ##################\")\n",
    "index = noise_metadata.shape[0] + event_metadata.shape[0]\n",
    "print(f\"index = {index}\")\n",
    "f1, t1, p1, s1, trace_name = dataset[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "833afc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_3c_wave_form(filename, trace_name):\n",
    "    with h5py.File(filename, 'r') as f:\n",
    "        return torch.tensor(f['data'][trace_name][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "65e7f5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size - Expected:6003 - Actual:6003\n",
      "Val size - Expected:353 - Actual:353\n",
      "Test size - Expected:706 - Actual:706\n",
      "Train first event data matches: True\n",
      "Train first target matches: True\n",
      "Train first noise data matches: True\n",
      "Train first noise target matches: True\n",
      "Train last event data matches: True\n",
      "Train last event target matches: True\n",
      "Train last noise data matches: True\n",
      "Train last noise target matches: True\n",
      "Val first event data matches: True\n",
      "Val first target matches: True\n",
      "Val first noise data matches: True\n",
      "Val first noise target matches: True\n",
      "Val last event data matches: True\n",
      "Val last event target matches: True\n",
      "Val last noise data matches: True\n",
      "Val last noise target matches: True\n",
      "Test first event data matches: True\n",
      "Test first target matches: True\n",
      "Test first noise data matches: True\n",
      "Test first noise target matches: True\n",
      "Test last event data matches: True\n",
      "Test last event target matches: True\n",
      "Test last noise data matches: True\n",
      "Test last noise target matches: True\n"
     ]
    }
   ],
   "source": [
    "#split_percentage=[train, validation, test]\n",
    "\n",
    "split_percentage=[0.85, 0.05, 0.1]\n",
    "\n",
    "train_dataset = InstanceDataset(event_hdf5_file, event_metadata_file, noise_hdf5_file, noise_metadata_file, \"binary\", split_index=0, split_percentage=split_percentage, padding_type=\"sample\", start_padding_value=100, end_padding_value=1000, target_phase=False, phase_padding=20)\n",
    "val_dataset = InstanceDataset(event_hdf5_file, event_metadata_file, noise_hdf5_file, noise_metadata_file, \"binary\", split_index=1, split_percentage=split_percentage, padding_type=\"sample\", start_padding_value=100, end_padding_value=1000, target_phase=False, phase_padding=20)\n",
    "test_dataset = InstanceDataset(event_hdf5_file, event_metadata_file, noise_hdf5_file, noise_metadata_file, \"binary\", split_index=2, split_percentage=split_percentage, padding_type=\"sample\", start_padding_value=100, end_padding_value=1000, target_phase=False, phase_padding=20)\n",
    "\n",
    "#Check dataset sizes\n",
    "train_events_count = math.floor(event_metadata.shape[0] * split_percentage[0])\n",
    "train_noise_count = math.floor(noise_metadata.shape[0] * split_percentage[0])\n",
    "print(f\"Train size - Expected:{train_events_count+train_noise_count} - Actual:{len(train_dataset)}\")\n",
    "\n",
    "\n",
    "val_events_count = math.floor(event_metadata.shape[0] * split_percentage[1])\n",
    "val_noise_count = math.floor(noise_metadata.shape[0] * split_percentage[1])\n",
    "print(f\"Val size - Expected:{val_events_count+val_noise_count} - Actual:{len(val_dataset)}\")\n",
    "\n",
    "\n",
    "test_events_count = math.floor(event_metadata.shape[0] * split_percentage[2])\n",
    "test_noise_count = math.floor(noise_metadata.shape[0] * split_percentage[2])\n",
    "print(f\"Test size - Expected:{test_events_count+test_noise_count} - Actual:{len(test_dataset)}\")\n",
    "\n",
    "\n",
    "#Check train dataset indexes\n",
    "absolute_index = 0\n",
    "data, target, p, s, name = train_dataset[0]\n",
    "actual_data = get_3c_wave_form(event_hdf5_file, event_metadata[\"trace_name\"].iloc[absolute_index])\n",
    "print(f\"Train first event data matches: {torch.equal(data, actual_data)}\")\n",
    "print(f\"Train first target matches: {target == 1}\")\n",
    "\n",
    "data, target, p, s, name = train_dataset[train_events_count]\n",
    "actual_data = get_3c_wave_form(noise_hdf5_file, noise_metadata[\"trace_name\"].iloc[absolute_index])\n",
    "print(f\"Train first noise data matches: {torch.equal(data, actual_data)}\")\n",
    "print(f\"Train first noise target matches: {target == 0}\")\n",
    "\n",
    "absolute_index = train_events_count - 1\n",
    "data, target, p, s, name = train_dataset[train_events_count - 1]\n",
    "actual_data = get_3c_wave_form(event_hdf5_file, event_metadata[\"trace_name\"].iloc[absolute_index])\n",
    "print(f\"Train last event data matches: {torch.equal(data, actual_data)}\")\n",
    "print(f\"Train last event target matches: {target == 1}\")\n",
    "\n",
    "absolute_index = train_noise_count - 1\n",
    "data, target, p, s, name = train_dataset[train_events_count + train_noise_count - 1]\n",
    "actual_data = get_3c_wave_form(noise_hdf5_file, noise_metadata[\"trace_name\"].iloc[absolute_index])\n",
    "print(f\"Train last noise data matches: {torch.equal(data, actual_data)}\")\n",
    "print(f\"Train last noise target matches: {target == 0}\")\n",
    "\n",
    "#Check validation dataset indexes\n",
    "absolute_index = train_events_count\n",
    "data, target, p, s, name = val_dataset[0]\n",
    "actual_data = get_3c_wave_form(event_hdf5_file, event_metadata[\"trace_name\"].iloc[absolute_index])\n",
    "print(f\"Val first event data matches: {torch.equal(data, actual_data)}\")\n",
    "print(f\"Val first target matches: {target == 1}\")\n",
    "\n",
    "absolute_index = train_noise_count\n",
    "data, target, p, s, name = val_dataset[val_events_count]\n",
    "actual_data = get_3c_wave_form(noise_hdf5_file, noise_metadata[\"trace_name\"].iloc[absolute_index])\n",
    "print(f\"Val first noise data matches: {torch.equal(data, actual_data)}\")\n",
    "print(f\"Val first noise target matches: {target == 0}\")\n",
    "\n",
    "absolute_index = train_events_count + val_events_count - 1\n",
    "data, target, p, s, name = val_dataset[val_events_count - 1]\n",
    "actual_data = get_3c_wave_form(event_hdf5_file, event_metadata[\"trace_name\"].iloc[absolute_index])\n",
    "print(f\"Val last event data matches: {torch.equal(data, actual_data)}\")\n",
    "print(f\"Val last event target matches: {target == 1}\")\n",
    "\n",
    "absolute_index = train_noise_count + val_noise_count - 1\n",
    "data, target, p, s, name = val_dataset[val_events_count + val_noise_count - 1]\n",
    "actual_data = get_3c_wave_form(noise_hdf5_file, noise_metadata[\"trace_name\"].iloc[absolute_index])\n",
    "print(f\"Val last noise data matches: {torch.equal(data, actual_data)}\")\n",
    "print(f\"Val last noise target matches: {target == 0}\")\n",
    "\n",
    "#Check test dataset indexes\n",
    "absolute_index = train_events_count + val_events_count\n",
    "data, target, p, s, name = test_dataset[0]\n",
    "actual_data = get_3c_wave_form(event_hdf5_file, event_metadata[\"trace_name\"].iloc[absolute_index])\n",
    "print(f\"Test first event data matches: {torch.equal(data, actual_data)}\")\n",
    "print(f\"Test first target matches: {target == 1}\")\n",
    "\n",
    "absolute_index = train_noise_count + val_noise_count\n",
    "data, target, p, s, name = test_dataset[test_events_count]\n",
    "actual_data = get_3c_wave_form(noise_hdf5_file, noise_metadata[\"trace_name\"].iloc[absolute_index])\n",
    "print(f\"Test first noise data matches: {torch.equal(data, actual_data)}\")\n",
    "print(f\"Test first noise target matches: {target == 0}\")\n",
    "\n",
    "absolute_index = train_events_count + val_events_count + test_events_count - 1\n",
    "data, target, p, s, name = test_dataset[test_events_count - 1]\n",
    "actual_data = get_3c_wave_form(event_hdf5_file, event_metadata[\"trace_name\"].iloc[absolute_index])\n",
    "print(f\"Test last event data matches: {torch.equal(data, actual_data)}\")\n",
    "print(f\"Test last event target matches: {target == 1}\")\n",
    "\n",
    "absolute_index = train_noise_count + val_noise_count + test_noise_count - 1\n",
    "data, target, p, s, name = test_dataset[test_events_count + test_noise_count - 1]\n",
    "actual_data = get_3c_wave_form(noise_hdf5_file, noise_metadata[\"trace_name\"].iloc[absolute_index])\n",
    "print(f\"Test last noise data matches: {torch.equal(data, actual_data)}\")\n",
    "print(f\"Test last noise target matches: {target == 0}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "95077e31-f845-4cf6-ba8e-b448ab281524",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ekabore/Documents/personal/school/H2024/IFT6759/earthquake/utils/plot.py:163: UserWarning: The figure layout has changed to tight\n",
      "  plt.tight_layout()\n"
     ]
    }
   ],
   "source": [
    "#split_percentage=[train, validation, test]\n",
    "\n",
    "split_percentage=[0.85, 0.05, 0.1]\n",
    "split_index = 0\n",
    "\n",
    "index = 102\n",
    "\n",
    "dataset = InstanceDataset(event_hdf5_file, event_metadata_file, noise_hdf5_file, noise_metadata_file, \"box_trapezoidal\", padding_type=\"percentage\", start_padding_value=1, end_padding_value=4, target_phase=True, phase_padding=200, split_index=split_index, split_percentage=split_percentage)\n",
    "\n",
    "\n",
    "trace_name = f'{event_metadata[\"trace_name\"].iloc[index]}-trapeze-sample'\n",
    "p_sample = event_metadata[\"trace_P_arrival_sample\"].iloc[index]\n",
    "s_sample = event_metadata[\"trace_S_arrival_sample\"].iloc[index]\n",
    "input, target, p_target, s_target, name = dataset[index]\n",
    "\n",
    "sample_size = input.shape[1]\n",
    "\n",
    "if np.isnan(p_sample):\n",
    "    p_sample = 0\n",
    "\n",
    "if np.isnan(s_sample):\n",
    "    s_sample = sample_size - 1\n",
    "\n",
    "s_sample = round(s_sample)\n",
    "\n",
    "plot_trace_prediction(trace_name, input, np.array([p_sample]), np.array([s_sample]), target, p_target, s_target, \"output/figures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88528d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ekabore/Documents/personal/school/H2024/IFT6759/earthquake/utils/plot.py:163: UserWarning: The figure layout has changed to tight\n",
      "  plt.tight_layout()\n",
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    }
   ],
   "source": [
    "#split_percentage=[train, validation, test]\n",
    "\n",
    "split_percentage=[0.85, 0.05, 0.1]\n",
    "split_index = 0\n",
    "\n",
    "index = 5\n",
    "\n",
    "dataset = InstanceDataset(event_hdf5_file, event_metadata_file, noise_hdf5_file, noise_metadata_file, \"binary\", padding_type=\"sample\", start_padding_value=40, end_padding_value=40, target_phase=False, phase_padding=100, split_index=split_index, split_percentage=split_percentage)\n",
    "\n",
    "\n",
    "trace_name = f'{event_metadata[\"trace_name\"].iloc[index]}-square-sample'\n",
    "p_sample = event_metadata[\"trace_P_arrival_sample\"].iloc[index]\n",
    "s_sample = event_metadata[\"trace_S_arrival_sample\"].iloc[index]\n",
    "input, target, p_target, s_target, _ = dataset[index]\n",
    "\n",
    "sample_size = input.shape[1]\n",
    "\n",
    "if np.isnan(p_sample):\n",
    "    p_sample = 0\n",
    "\n",
    "if np.isnan(s_sample):\n",
    "    s_sample = sample_size - 1\n",
    "\n",
    "s_sample = round(s_sample)\n",
    "\n",
    "plot_trace_prediction(trace_name, input, np.array([p_sample]), np.array([s_sample]), target, p_target, s_target, \"output/figures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d226129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index=0\n",
      "5\n",
      "torch.Size([3, 12000])\n",
      "tensor(1)\n",
      "tensor(-1)\n",
      "tensor(-1)\n",
      "Print data are good\n",
      "=============\n",
      "y: 1 - trace name: 11269271.IV.GUMA..HH\n",
      "Data is signal and matches: True\n",
      "=============\n",
      "y: 0 - trace name: 20191231T182220.IV.CESX..HH\n",
      "Data is noise and matches: True\n",
      "=============\n",
      "y: 1 - trace name: 11041921.IV.MURB..HH\n",
      "Data is signal and matches: True\n",
      "=============\n",
      "y: 1 - trace name: 11190131.IV.ATCC..HN\n",
      "Data is signal and matches: True\n",
      "=============\n",
      "y: 1 - trace name: 11105061.IV.NRCA..HN\n",
      "Data is signal and matches: True\n",
      "=============\n",
      "y: 1 - trace name: 11237341.IV.LMD..HH\n",
      "Data is signal and matches: True\n",
      "=============\n",
      "y: 1 - trace name: 11079441.IV.TERO..HH\n",
      "Data is signal and matches: True\n",
      "=============\n",
      "y: 1 - trace name: 11278861.IV.AIO..HH\n",
      "Data is signal and matches: True\n",
      "=============\n",
      "y: 1 - trace name: 1115921.IV.MSSA..HH\n",
      "Data is signal and matches: True\n",
      "=============\n",
      "y: 1 - trace name: 1110151.IV.SSFR..HN\n",
      "Data is signal and matches: True\n"
     ]
    }
   ],
   "source": [
    "#split_percentage=[train, validation, test]\n",
    "\n",
    "split_percentage=[0.85, 0.05, 0.1]\n",
    "split_index = 0\n",
    "\n",
    "dataset = InstanceDataset(event_hdf5_file, shuffled_event_metadata_file, noise_hdf5_file, shuffled_noise_metadata_file, \"binary\", padding_type=\"sample\", start_padding_value=40, end_padding_value=40, target_phase=False, phase_padding=100, remove_empty_phase=False, split_index=split_index, split_percentage=split_percentage)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=10, shuffle=True, num_workers=2\n",
    ")\n",
    "\n",
    "for i, batch in enumerate(train_loader):\n",
    "    print(f'index={i}')\n",
    "    #batch[0][i] == tensor 3x12000 of the data of the i element\n",
    "    #batch[1][i] == target of i element\n",
    "    #batch[2][i] == p_target of i element\n",
    "    #batch[3][i] == s_target of i element\n",
    "    print(len(batch))\n",
    "    print(batch[0][0].size())\n",
    "    print(batch[1][0])\n",
    "    print(batch[2][0])\n",
    "    print(batch[3][0])\n",
    "\n",
    "    data = batch[0]\n",
    "    targets = batch[1]\n",
    "    trace_names = batch[4]\n",
    "    break\n",
    "\n",
    "print(\"Print data are good\")\n",
    "\n",
    "for i in range(data.shape[0]):\n",
    "    print(\"=============\")\n",
    "    trace_name = trace_names[i]\n",
    "    y = targets[i]\n",
    "    print(f\"y: {y} - trace name: {trace_name}\")\n",
    "    if y == 0:\n",
    "        actual_data = get_3c_wave_form(noise_hdf5_file, trace_name)\n",
    "        print(f\"Data is noise and matches: {torch.equal(data[i], actual_data)}\")\n",
    "    else:\n",
    "        actual_data = get_3c_wave_form(event_hdf5_file, trace_name)\n",
    "        print(f\"Data is signal and matches: {torch.equal(data[i], actual_data)}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "252f5c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=63, data size=64, hasNoise=True, hasEvent=True everything is good!\n",
      "index=26, data size=27, hasNoise=True, hasEvent=True everything is good!\n"
     ]
    }
   ],
   "source": [
    "#Check data are loading properly\n",
    "\n",
    "#split_percentage=[train, validation, test]\n",
    "\n",
    "split_percentage=[0.91, 0.04, 0.05]\n",
    "split_index = 0\n",
    "\n",
    "dataset = InstanceDataset(event_hdf5_file, event_metadata_file, noise_hdf5_file, noise_metadata_file, \"binary\", padding_type=\"sample\", start_padding_value=40, end_padding_value=40, target_phase=False, phase_padding=100, remove_empty_phase=True, split_index=split_index, split_percentage=split_percentage)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=64, shuffle=True, num_workers=2\n",
    ")\n",
    "should_continue = True\n",
    "\n",
    "for i, batch in enumerate(train_loader):\n",
    "\n",
    "    data = batch[0]\n",
    "    targets = batch[1]\n",
    "    trace_names = batch[4]\n",
    "\n",
    "    hasEvent = False\n",
    "    hasNoise = False\n",
    "\n",
    "    for i in range(data.shape[0]):\n",
    "        trace_name = trace_names[i]\n",
    "        y = targets[i]\n",
    "        if y == 0:\n",
    "            actual_data = get_3c_wave_form(noise_hdf5_file, trace_name)\n",
    "            hasNoise = True\n",
    "            if not torch.equal(data[i], actual_data):\n",
    "                print(f'index={i}, data size={data.shape[0]}, batch_i={i}, y={y}, {trace_name} is not loaded properly')\n",
    "                should_continue = False\n",
    "                break\n",
    "        else:\n",
    "            actual_data = get_3c_wave_form(event_hdf5_file, trace_name)\n",
    "            hasEvent = True\n",
    "            if not torch.equal(data[i], actual_data):\n",
    "                print(f'index={i}, data size={data.shape[0]}, batch_i={i}, y={y}, {trace_name} is not loaded properly')\n",
    "                should_continue = False\n",
    "                break\n",
    "    \n",
    "    if not should_continue:\n",
    "        break\n",
    "    \n",
    "    print(f'index={i}, data size={data.shape[0]}, hasNoise={hasNoise}, hasEvent={hasEvent} everything is good!')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32f497b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=127, data size=128, hasNoise=True, hasEvent=True everything is good!\n",
      "index=119, data size=120, hasNoise=True, hasEvent=True everything is good!\n"
     ]
    }
   ],
   "source": [
    "#Check data are loading properly\n",
    "\n",
    "#split_percentage=[train, validation, test]\n",
    "\n",
    "split_percentage=[1]\n",
    "split_index = 0\n",
    "\n",
    "dataset = InstanceDataset(event_hdf5_file, event_metadata_file, noise_hdf5_file, noise_metadata_file, \"binary\", padding_type=\"sample\", start_padding_value=40, end_padding_value=40, target_phase=False, phase_padding=100, remove_empty_phase=False, split_index=split_index, split_percentage=split_percentage)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=128, shuffle=True, num_workers=2\n",
    ")\n",
    "should_continue = True\n",
    "\n",
    "for i, batch in enumerate(train_loader):\n",
    "\n",
    "    data = batch[0]\n",
    "    targets = batch[1]\n",
    "    trace_names = batch[4]\n",
    "\n",
    "    hasEvent = False\n",
    "    hasNoise = False\n",
    "\n",
    "    for i in range(data.shape[0]):\n",
    "        trace_name = trace_names[i]\n",
    "        y = targets[i]\n",
    "        if y == 0:\n",
    "            actual_data = get_3c_wave_form(noise_hdf5_file, trace_name)\n",
    "            hasNoise = True\n",
    "            if not torch.equal(data[i], actual_data):\n",
    "                print(f'index={i}, data size={data.shape[0]}, batch_i={i}, y={y}, {trace_name} is not loaded properly')\n",
    "                should_continue = False\n",
    "                break\n",
    "        else:\n",
    "            actual_data = get_3c_wave_form(event_hdf5_file, trace_name)\n",
    "            hasEvent = True\n",
    "            if not torch.equal(data[i], actual_data):\n",
    "                print(f'index={i}, data size={data.shape[0]}, batch_i={i}, y={y}, {trace_name} is not loaded properly')\n",
    "                should_continue = False\n",
    "                break\n",
    "    \n",
    "    if not should_continue:\n",
    "        break\n",
    "    \n",
    "    print(f'index={i}, data size={data.shape[0]}, hasNoise={hasNoise}, hasEvent={hasEvent} everything is good!')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6055f647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator - data shape = torch.Size([12000, 3])\n",
      "Generator - detection = torch.Size([12000])\n",
      "Generator - p_phase = torch.Size([12000])\n",
      "Generator - s_phase = torch.Size([12000])\n",
      "Generator - data shape = torch.Size([12000, 3])\n",
      "Generator - detection = torch.Size([12000])\n",
      "Generator - p_phase = torch.Size([12000])\n",
      "Generator - s_phase = torch.Size([12000])\n",
      "Generator - data shape = torch.Size([12000, 3])\n",
      "Generator - detection = torch.Size([12000])\n",
      "Generator - p_phase = torch.Size([12000])\n",
      "Generator - s_phase = torch.Size([12000])\n",
      "Generator - data shape = torch.Size([12000, 3])\n",
      "Generator - detection = torch.Size([12000])\n",
      "Generator - p_phase = torch.Size([12000])\n",
      "Generator - s_phase = torch.Size([12000])\n"
     ]
    }
   ],
   "source": [
    "split_percentage=[1]\n",
    "split_index = 0\n",
    "\n",
    "dataset = InstanceDataset(event_hdf5_file, event_metadata_file, noise_hdf5_file, noise_metadata_file, \"box_trapezoidal\", padding_type=\"percentage\", start_padding_value=0, end_padding_value=0.4, target_phase=True, phase_padding=40, remove_empty_phase=True, split_index=split_index, split_percentage=split_percentage, norm_mode='max')\n",
    "\n",
    "datagenerator = DataGenerator(dataset, 4, True)\n",
    "\n",
    "batch = datagenerator[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b147602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "def normalize(data, mode = 'max'):  \n",
    "    'Normalize waveforms'\n",
    "    \n",
    "    data -= np.mean(data, axis=1, keepdims=True)\n",
    "    if mode == 'max':\n",
    "        max_data = np.max(data, axis=1, keepdims=True)\n",
    "        assert(max_data.shape[-2] == data.shape[-2])\n",
    "        max_data[max_data == 0] = 1\n",
    "        data /= max_data              \n",
    "\n",
    "    elif mode == 'std':               \n",
    "        std_data = np.std(data, axis=1, keepdims=True)\n",
    "        assert(std_data.shape[-2] == data.shape[-2])\n",
    "        std_data[std_data == 0] = 1\n",
    "        data /= std_data\n",
    "    return data\n",
    "\n",
    "test = np.array([[11.0, 2.0, -4, 7], [-3, 6, 8, -7]])\n",
    "# test2 = np.ones(2)\n",
    "\n",
    "# test[1] = test2[..., np.newaxis]\n",
    "\n",
    "# test\n",
    "\n",
    "# print(test2.shape)\n",
    "# test2[..., np.newaxis]\n",
    "\n",
    "\n",
    "print(np.mean(test, axis=1, keepdims=True))\n",
    "\n",
    "# normalize(test, 'std')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2472, -0.3563, -1.4254,  0.5345],\n",
       "        [-0.6447,  0.8058,  1.1282, -1.2893]], dtype=torch.float64)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize(data, mode = 'max'):  \n",
    "    'Normalize waveforms'\n",
    "    \n",
    "    data -= torch.mean(data, axis=1, keepdims=True)\n",
    "    if mode == 'max':\n",
    "        max_data = torch.max(data, axis=1, keepdims=True).values\n",
    "        assert(max_data.shape[-2] == data.shape[-2])\n",
    "        max_data[max_data == 0] = 1\n",
    "        data /= max_data              \n",
    "\n",
    "    elif mode == 'std':               \n",
    "        std_data = torch.std(data, axis=1, keepdims=True, unbiased=False)\n",
    "        assert(std_data.shape[-2] == data.shape[-2])\n",
    "        std_data[std_data == 0] = 1\n",
    "        data /= std_data\n",
    "    return data\n",
    "\n",
    "test = np.array([[11.0, 2.0, -4, 7], [-3, 6, 8, -7]])\n",
    "test = torch.from_numpy(test)\n",
    "\n",
    "\n",
    "# print(torch.std(test, axis=1, keepdims=True, unbiased=False))\n",
    "\n",
    "normalize(test, 'std')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "38c38a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old event shape=(10000, 11)\n",
      "new event shape=(10000, 11)\n",
      "old noise shape=(1000, 43)\n",
      "old noise shape=(1000, 43)\n"
     ]
    }
   ],
   "source": [
    "def _shuffle_events(event_metadata_file, noise_metadata_file, random_state, output_event_metadata_file, output_noise_metadata_file):\n",
    "    \n",
    "    \"\"\" \n",
    "    \n",
    "    Split the list of input data into training, validation, and test set.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    event_metadata_file: str\n",
    "        path to event metadata file \n",
    "    \n",
    "    noise_metadata_file: str\n",
    "        path to event metadata file \n",
    "\n",
    "    random_state: int\n",
    "        random state for reproducibility\n",
    "\n",
    "    output_event_metadata_file: bool\n",
    "       Path to the output event metadata file\n",
    "\n",
    "    output_noise_metadata_file: str\n",
    "       Path to the output noise metadata file\n",
    "              \n",
    "    Returns\n",
    "    -------   \n",
    "    (output_event_metadata_file, output_noise_metadata_file)\n",
    "    \"\"\"       \n",
    "\n",
    "    noise_metadata = pd.read_csv(noise_metadata_file)\n",
    "    event_metadata = pd.read_csv(event_metadata_file, usecols=[\"trace_name\", 'trace_start_time', 'trace_P_arrival_time', 'trace_S_arrival_time', 'trace_P_arrival_sample', 'trace_S_arrival_sample', 'trace_GPD_P_number', 'trace_GPD_S_number', 'trace_EQT_P_number', 'trace_EQT_S_number', 'trace_EQT_number_detections'])\n",
    "\n",
    "    event_metadata = event_metadata.sample(frac=1, random_state=random_state) \n",
    "    noise_metadata =noise_metadata.sample(frac=1, random_state=random_state) \n",
    "\n",
    "    event_metadata.to_csv(output_event_metadata_file, index=False)\n",
    "    noise_metadata.to_csv(output_noise_metadata_file, index=False)\n",
    "\n",
    "    return output_event_metadata_file, output_noise_metadata_file \n",
    "\n",
    "new_event, new_noise = _shuffle_events(\n",
    "    event_metadata_file=event_metadata_file,\n",
    "    noise_metadata_file=noise_metadata_file,\n",
    "    random_state=9,\n",
    "    output_event_metadata_file='temp/event_metadata.csv',\n",
    "    output_noise_metadata_file='temp/noise_metadata.csv'\n",
    ")\n",
    "\n",
    "noise_metadata = pd.read_csv(noise_metadata_file)\n",
    "event_metadata = pd.read_csv(event_metadata_file, usecols=[\"trace_name\", 'trace_start_time', 'trace_P_arrival_time', 'trace_S_arrival_time', 'trace_P_arrival_sample', 'trace_S_arrival_sample', 'trace_GPD_P_number', 'trace_GPD_S_number', 'trace_EQT_P_number', 'trace_EQT_S_number', 'trace_EQT_number_detections'])\n",
    "\n",
    "\n",
    "new_noise_m = pd.read_csv(new_noise)\n",
    "new_event_m = pd.read_csv(new_event, usecols=[\"trace_name\", 'trace_start_time', 'trace_P_arrival_time', 'trace_S_arrival_time', 'trace_P_arrival_sample', 'trace_S_arrival_sample', 'trace_GPD_P_number', 'trace_GPD_S_number', 'trace_EQT_P_number', 'trace_EQT_S_number', 'trace_EQT_number_detections'])\n",
    "\n",
    "print(f\"old event shape={event_metadata.shape}\")\n",
    "print(f\"new event shape={new_event_m.shape}\")\n",
    "print(f\"old noise shape={noise_metadata.shape}\")\n",
    "print(f\"old noise shape={new_noise_m.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
