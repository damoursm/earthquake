{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e0d0862-b522-4cb0-b69a-fcce2bb13898",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from __future__ import print_function\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b065d7bf-5081-4bd1-bca5-d96425f2215c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fonction qui permet d\"extraire les signaux events ou noise en trois dimention ex:(1000, 3, 12000) ainsi que le trace_name correspondant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79704c02-9ded-48d2-8750-4bb50eb9f04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filename):\n",
    "    with h5py.File(filename, 'r') as f:\n",
    "        if 'data' in f:\n",
    "            data = f['data']\n",
    "            if isinstance(data, h5py.Dataset):\n",
    "                print(\"Type du dataset 'data':\", type(data))\n",
    "                print(\"Forme du dataset 'data':\", data.shape)\n",
    "                data_matrix = data[:]  \n",
    "                print(\"Matrice de données:\", data_matrix)\n",
    "                return data_matrix\n",
    "            elif isinstance(data, h5py.Group):\n",
    "                data_list = []\n",
    "                trace_names = []\n",
    "                for key in data.keys():\n",
    "                    dataset = data[key]\n",
    "                    if isinstance(dataset, h5py.Dataset):\n",
    "                        # Vérifier la forme des données extraites\n",
    "                        if dataset.ndim == 2:\n",
    "                            \n",
    "                            #print(dataset.shape[0])\n",
    "                            data_array = np.zeros((1, dataset.shape[0], dataset.shape[1]), dtype=np.float32)  # Créer une matrice vide de trois dimensions\n",
    "                            for i in range(2):  # Boucle à travers les canaux\n",
    "                                data_array[:,i,:] = dataset[i,:]  # Récupérer les valeurs des trois canaux existants\n",
    "                            data_list.append(data_array)\n",
    "                            #trace_names.append(key)\n",
    "                            # Concaténer les noms de clés avec un tableau de zéros\n",
    "                            trace_names = np.concatenate([np.array([key] ).reshape(-1, 1) for key in data.keys()])\n",
    "                            # Remodeler en un vecteur colonne\n",
    "                            trace_names=trace_names.reshape(-1, 1)                          \n",
    "                            #trace_names['trace_names']=trace_names\n",
    "                        \n",
    "                        else:\n",
    "                            print(f\"Les données pour la clé '{key}' ne sont pas sous forme de tableau 2D.\")\n",
    "                if data_list:\n",
    "                    stacked_data_matrix = np.stack(data_list, axis=0)\n",
    "                    #Redimensionnement des données\n",
    "                    stacked_data_matrix = np.squeeze(stacked_data_matrix, axis=1)\n",
    "\n",
    "                    # Vérification du nouveau shape\n",
    "                    #print(data_resized.shape) \n",
    "                    return stacked_data_matrix, trace_names\n",
    "\n",
    "# Utilisation de la fonction, pour charger et convertir le dataset à partir du fichier HDF5\n",
    "filename1 = 'Instance_noise_1k.hdf5'\n",
    "noise_matrix, trace_names_noise = load_dataset(filename1)\n",
    "\n",
    "filename2 = 'Instance_events_counts_10k.hdf5'\n",
    "events_matrix, trace_names_events = load_dataset(filename2)\n",
    "\n",
    "#print (events_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea6ed2a-4ddd-419b-8f83-b2e575b4e7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'une colonne 'source_type' avec des valeurs constantes de 0\n",
    "source_type_n = np.zeros((len(trace_names_noise), 1), dtype=int)\n",
    "\n",
    "# Création d'une colonne 'source_type' avec des valeurs constantes de 1\n",
    "source_type_e = np.ones((len(trace_names_events), 1), dtype=int)\n",
    "\n",
    "# Concaténation de la colonne 'source_type' avec trace_names_noise\n",
    "trace_names_n = np.concatenate((trace_names_noise, source_type_n), axis=1)\n",
    "trace_names_e = np.concatenate((trace_names_events, source_type_e), axis=1)\n",
    "\n",
    "# Vérification de la forme de la matrice résultante\n",
    "print(trace_names_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413a14ec-d38a-44e3-94fd-4cd0a7eb6a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Préparation des données dentrainement, validation et de test\n",
    "Concatenation de données noise et events pour chaque partie.\n",
    "la meme chose est faite pour les trace_names qui vont avec les données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb471a3c-8e0a-49e6-bd4e-3d1da43d0565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préparation des données d'entrainement, validation et de test\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def prepa_donnees (events_matrix, trace_names_e, noise_matrix,trace_names_n, split_size= split_size)\n",
    "    # Diviser les données de bruit en ensemble d'entraînement et ensemble de test\n",
    "    df_noise_train1, df_noise_test = train_test_split(noise_matrix, test_size=split_size, random_state=42)\n",
    "    df_noise_train, df_noise_validation = train_test_split(df_noise_train1, test_size=split_size, random_state=42)\n",
    "\n",
    "    trace_noise1, trace_noise_test =train_test_split(trace_names_n, test_size=split_size, random_state=42)\n",
    "    trace_noise_train, trace_noise_val = train_test_split(trace_noise1, test_size=split_size, random_state=42)\n",
    "\n",
    "    trace_events1, trace_events_test =train_test_split(trace_names_e, test_size=split_size, random_state=42)\n",
    "    trace_events_train, trace_events_val = train_test_split(trace_events1, test_size=split_size, random_state=42)\n",
    "\n",
    "\n",
    "    df_train1, df_test = train_test_split(events_matrix, test_size=split_size, random_state=42)\n",
    "    df_train, df_validation = train_test_split(df_train1, test_size=split_size, random_state=42)\n",
    "\n",
    "\n",
    "    # Concaténer df_noise_train avec df_train\n",
    "    df_train_concat = np.concatenate([df_train, df_noise_train], axis=0)\n",
    "    trace_train = np.concatenate([trace_events_train, trace_noise_train], axis=0)\n",
    "    trace_train = trace_train[:,1].astype(int)\n",
    "\n",
    "    # Concaténer df_noise_test avec df_test\n",
    "    df_test_concat = np.concatenate([df_test, df_noise_test], axis=0)\n",
    "    trace_test= np.concatenate([trace_events_test, trace_noise_test], axis=0)\n",
    "    # Convertir les étiquettes en entiers\n",
    "    trace_test = trace_test[:,1].astype(int)\n",
    "\n",
    "    #print (df_test_concat)\n",
    "    # Concaténer df_noise_validation avec df_validation\n",
    "    df_val_concat = np.concatenate([df_validation, df_noise_validation], axis=0)\n",
    "    trace_val= np.concatenate([trace_events_val, trace_noise_val], axis=0)\n",
    "    trace_val = trace_val[:,1].astype(int)\n",
    "    \n",
    "    return df_train_concat, df_val_concat, df_test_concat, trace_train, trace_test, trace_val\n",
    "\n",
    "df_train_concat, df_val_concat, df_test_concat, trace_train, trace_test, trace_val= prepa_donnees (events_matrix, trace_names_e, noise_matrix,trace_names_n,split_size= 0.25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8679cc-05f2-44c0-8dfe-f0e5595b29c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84af2eab-0cc2-4247-922a-76a85da7da08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalisation(df_train_concat, df_val_concat, df_test_concat)\n",
    "\n",
    "    # Normaliser toute la donnée\n",
    "    mean_t = np.mean(df_train_concat)\n",
    "    std_t = np.std(df_train_concat)\n",
    "    norm_data_t = (df_train_concat - mean_t) / std_t\n",
    "\n",
    "    mean_v = np.mean(df_val_concat)\n",
    "    std_v = np.std(df_val_concat)\n",
    "    norm_data_v = (df_val_concat - mean_v) / std_v\n",
    "\n",
    "    mean_ts = np.mean(df_test_concat)\n",
    "    std_ts = np.std(df_test_concat)\n",
    "    norm_data_ts = (df_test_concat - mean_ts) / std_ts\n",
    "    \n",
    "    return norm_data_t, norm_data_v, norm_data_ts\n",
    "\n",
    "norm_data_t, norm_data_v, norm_data_ts= normalisation(df_train_concat, df_val_concat, df_test_concat)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e16367e-7cc9-4200-9225-4788e3b28145",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5116be-7e29-4296-8fbb-24ae7e30ade2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
