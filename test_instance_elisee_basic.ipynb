{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092318e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################## Purpose ##############################\n",
    "# Show how to use instance dataset to train a CNN binary classifier #\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe1cf195-5793-4d30-9912-4abec6969214",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataloaders.EventDetectionInstanceDataset import EventDetectionInstanceDataset\n",
    "from utils.dataloaders.InstanceDataset import InstanceDataset\n",
    "from utils.generic_trainer import train_detection_only\n",
    "from utils.plot import plot_error_and_accuracy\n",
    "from utils.correct_counter import correct\n",
    "import shutil\n",
    "\n",
    "# Autre proposition pour le CNN\n",
    "\n",
    "from nn.cnn import CNN\n",
    "from nn.conv_block import ConvBlock\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0591268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: Train=9350 - Val=550 - Test=1100\n",
      "Earthquake Data shape: torch.Size([3, 12000])\n",
      "Earthquake Target: 1\n",
      "Noise Data shape: torch.Size([3, 12000])\n",
      "Noise Target: 0\n"
     ]
    }
   ],
   "source": [
    "event_hdf5_file = \"data/instance_samples/Instance_events_counts_10k.hdf5\"\n",
    "event_metadata_file = \"data/instance_samples/metadata_Instance_events_10k.csv\"\n",
    "noise_hdf5_file = \"data/instance_samples/Instance_noise_1k.hdf5\"\n",
    "noise_metadata_file = \"data/instance_samples/metadata_Instance_noise_1k.csv\"\n",
    "\n",
    "split_percentage=[0.85, 0.05, 0.1]\n",
    "\n",
    "train_dataset = EventDetectionInstanceDataset(event_hdf5_file, event_metadata_file, noise_hdf5_file, noise_metadata_file, \"binary\", split_index=0, split_percentage=split_percentage, padding_type=\"sample\", padding_value=100)\n",
    "val_dataset = EventDetectionInstanceDataset(event_hdf5_file, event_metadata_file, noise_hdf5_file, noise_metadata_file, \"binary\", split_index=1, split_percentage=split_percentage, padding_type=\"sample\", padding_value=100)\n",
    "test_dataset = EventDetectionInstanceDataset(event_hdf5_file, event_metadata_file, noise_hdf5_file, noise_metadata_file, \"binary\", split_index=2, split_percentage=split_percentage, padding_type=\"sample\", padding_value=100)\n",
    "\n",
    "print(f\"Dataset size: Train={len(train_dataset)} - Val={len(val_dataset)} - Test={len(test_dataset)}\")\n",
    "\n",
    "data, target = train_dataset[0]\n",
    "print(f\"Earthquake Data shape: {data.shape}\")\n",
    "print(f\"Earthquake Target: {target}\")\n",
    "\n",
    "\n",
    "data, target = train_dataset[len(train_dataset) - 1]\n",
    "print(f\"Noise Data shape: {data.shape}\")\n",
    "print(f\"Noise Target: {target}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac304372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv_layers): Sequential(\n",
      "    (0): ConvBlock(\n",
      "      (block): Sequential(\n",
      "        (0): Conv1d(3, 8, kernel_size=(11,), stride=(1,))\n",
      "        (1): LeakyReLU(negative_slope=0.01)\n",
      "        (2): Identity()\n",
      "        (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (4): Dropout(p=0.4, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): ConvBlock(\n",
      "      (block): Sequential(\n",
      "        (0): Conv1d(8, 16, kernel_size=(9,), stride=(1,))\n",
      "        (1): LeakyReLU(negative_slope=0.01)\n",
      "        (2): Identity()\n",
      "        (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (4): Dropout(p=0.4, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): ConvBlock(\n",
      "      (block): Sequential(\n",
      "        (0): Conv1d(16, 32, kernel_size=(7,), stride=(1,))\n",
      "        (1): LeakyReLU(negative_slope=0.01)\n",
      "        (2): Identity()\n",
      "        (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (4): Dropout(p=0.4, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): ConvBlock(\n",
      "      (block): Sequential(\n",
      "        (0): Conv1d(32, 64, kernel_size=(5,), stride=(1,))\n",
      "        (1): LeakyReLU(negative_slope=0.01)\n",
      "        (2): Identity()\n",
      "        (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (4): Dropout(p=0.4, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): ConvBlock(\n",
      "      (block): Sequential(\n",
      "        (0): Conv1d(64, 128, kernel_size=(3,), stride=(1,))\n",
      "        (1): LeakyReLU(negative_slope=0.01)\n",
      "        (2): Identity()\n",
      "        (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (4): Dropout(p=0.4, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (aggregation): AdaptiveAvgPool1d(output_size=1)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Dropout(p=0.4, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (4): LeakyReLU(negative_slope=0.01)\n",
      "    (5): Dropout(p=0.4, inplace=False)\n",
      "    (6): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Dropout(p=0.4, inplace=False)\n",
      "    (9): Linear(in_features=32, out_features=2, bias=True)\n",
      "    (10): LeakyReLU(negative_slope=0.01)\n",
      "    (11): Dropout(p=0.4, inplace=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "Shape of intermediate features through the network:\n",
      "Input: (5, 3, 12000)\n",
      "Test ======\n",
      "  ->  ConvBlock(\n",
      "  (block): Sequential(\n",
      "    (0): Conv1d(3, 8, kernel_size=(11,), stride=(1,))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Identity()\n",
      "    (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Dropout(p=0.4, inplace=False)\n",
      "  )\n",
      ") -> (5, 8, 5995)\n",
      "Test ======\n",
      "  ->  ConvBlock(\n",
      "  (block): Sequential(\n",
      "    (0): Conv1d(8, 16, kernel_size=(9,), stride=(1,))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Identity()\n",
      "    (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Dropout(p=0.4, inplace=False)\n",
      "  )\n",
      ") -> (5, 16, 2993)\n",
      "Test ======\n",
      "  ->  ConvBlock(\n",
      "  (block): Sequential(\n",
      "    (0): Conv1d(16, 32, kernel_size=(7,), stride=(1,))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Identity()\n",
      "    (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Dropout(p=0.4, inplace=False)\n",
      "  )\n",
      ") -> (5, 32, 1493)\n",
      "Test ======\n",
      "  ->  ConvBlock(\n",
      "  (block): Sequential(\n",
      "    (0): Conv1d(32, 64, kernel_size=(5,), stride=(1,))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Identity()\n",
      "    (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Dropout(p=0.4, inplace=False)\n",
      "  )\n",
      ") -> (5, 64, 744)\n",
      "Test ======\n",
      "  ->  ConvBlock(\n",
      "  (block): Sequential(\n",
      "    (0): Conv1d(64, 128, kernel_size=(3,), stride=(1,))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Identity()\n",
      "    (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Dropout(p=0.4, inplace=False)\n",
      "  )\n",
      ") -> (5, 128, 371)\n",
      "  ->  AdaptiveAvgPool1d(output_size=1) -> (5, 128, 1)\n",
      "  ->  Reshape -> (5, 128)\n",
      "  ->  Linear(in_features=128, out_features=128, bias=True) -> (5, 128)\n",
      "  ->  Linear(in_features=128, out_features=64, bias=True) -> (5, 64)\n",
      "  ->  Linear(in_features=64, out_features=32, bias=True) -> (5, 32)\n",
      "  ->  Linear(in_features=32, out_features=2, bias=True) -> (5, 2)\n"
     ]
    }
   ],
   "source": [
    "#Testing model shape and layers compatibility\n",
    "\n",
    "model = CNN(\n",
    "    input_channels=3,\n",
    "    conv_channels= [\n",
    "        8, 16, 32, 64, 128\n",
    "    ], kernel_sizes=[\n",
    "        11, 9, 7, 5, 3\n",
    "    ], mlp_layers=[\n",
    "        128, 64, 32, 2\n",
    "    ],\n",
    "    dropout=0.4\n",
    ")\n",
    "\n",
    "print(model)\n",
    "print(\"\\nShape of intermediate features through the network:\")\n",
    "dummy = []\n",
    "for i in range(5):\n",
    "    data, target = train_dataset[i]\n",
    "    dummy.append(data)\n",
    "\n",
    "dummy = torch.stack(dummy)\n",
    "\n",
    "print(f\"Input: {tuple(dummy.shape)}\")\n",
    "\n",
    "for layer in model.conv_layers:\n",
    "    print(\"Test ======\")\n",
    "    dummy = layer(dummy)\n",
    "    if isinstance(layer, ConvBlock):\n",
    "        print(f\"  ->  {layer} -> {tuple(dummy.shape)}\")\n",
    "dummy = model.aggregation(dummy)\n",
    "print(f\"  ->  {model.aggregation} -> {tuple(dummy.shape)}\")\n",
    "dummy = dummy.view(dummy.size(0), -1)\n",
    "print(f\"  ->  Reshape -> {tuple(dummy.shape)}\")\n",
    "for layer in model.mlp:\n",
    "    dummy = layer(dummy)\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        print(f\"  ->  {layer} -> {tuple(dummy.shape)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ecfb0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: torch.Size([5, 3, 12000])\n",
      "######### Actual class ###############\n",
      "tensor([1, 1, 1, 1, 1])\n",
      "######### Predictions logit ###############\n",
      "tensor([[-0.0000e+00,  0.0000e+00],\n",
      "        [-4.2490e-02,  0.0000e+00],\n",
      "        [-1.2359e-03,  4.9498e-01],\n",
      "        [-0.0000e+00, -2.1641e-03],\n",
      "        [-0.0000e+00,  1.1173e+01]], grad_fn=<MulBackward0>)\n",
      "Loss: 0.507004976272583\n",
      "Total correct: 3.0\n"
     ]
    }
   ],
   "source": [
    "#Testing dataset, dataloader, model and loss compatibility\n",
    "\n",
    "model = CNN(\n",
    "    input_channels=3,\n",
    "    conv_channels= [\n",
    "        8, 16, 32, 64, 128\n",
    "    ], kernel_sizes=[\n",
    "        11, 9, 7, 5, 3\n",
    "    ], mlp_layers=[\n",
    "        128, 64, 32, 2\n",
    "    ],\n",
    "    dropout=0.4\n",
    ")\n",
    "\n",
    "batch = []\n",
    "y = []\n",
    "batch_size=5\n",
    "\n",
    "# initialize loaders\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, num_workers=2\n",
    ")\n",
    "\n",
    "data = None\n",
    "actual_event = None\n",
    "\n",
    "for i, batch in enumerate(train_loader):\n",
    "    data = batch[0]\n",
    "    actual_event = batch[1]\n",
    "    break\n",
    "\n",
    "\n",
    "print(f\"Input: {data.shape}\")\n",
    "print(f\"######### Actual class ###############\")\n",
    "print(actual_event)\n",
    "\n",
    "pred = model(data)\n",
    "\n",
    "print(f\"######### Predictions logit ###############\")\n",
    "print(pred)\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"Loss: {loss(pred, actual_event)}\")\n",
    "print(f\"Total correct: {correct(pred, actual_event)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c28f8d06-e5c4-4b7a-8663-c80255a5f29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv_layers): Sequential(\n",
      "    (0): ConvBlock(\n",
      "      (block): Sequential(\n",
      "        (0): Conv1d(3, 8, kernel_size=(11,), stride=(1,))\n",
      "        (1): LeakyReLU(negative_slope=0.01)\n",
      "        (2): Identity()\n",
      "        (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (4): Dropout(p=0.4, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): ConvBlock(\n",
      "      (block): Sequential(\n",
      "        (0): Conv1d(8, 16, kernel_size=(9,), stride=(1,))\n",
      "        (1): LeakyReLU(negative_slope=0.01)\n",
      "        (2): Identity()\n",
      "        (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (4): Dropout(p=0.4, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): ConvBlock(\n",
      "      (block): Sequential(\n",
      "        (0): Conv1d(16, 32, kernel_size=(7,), stride=(1,))\n",
      "        (1): LeakyReLU(negative_slope=0.01)\n",
      "        (2): Identity()\n",
      "        (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (4): Dropout(p=0.4, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): ConvBlock(\n",
      "      (block): Sequential(\n",
      "        (0): Conv1d(32, 64, kernel_size=(5,), stride=(1,))\n",
      "        (1): LeakyReLU(negative_slope=0.01)\n",
      "        (2): Identity()\n",
      "        (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (4): Dropout(p=0.4, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): ConvBlock(\n",
      "      (block): Sequential(\n",
      "        (0): Conv1d(64, 128, kernel_size=(3,), stride=(1,))\n",
      "        (1): LeakyReLU(negative_slope=0.01)\n",
      "        (2): Identity()\n",
      "        (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (4): Dropout(p=0.4, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (aggregation): AdaptiveAvgPool1d(output_size=1)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Dropout(p=0.4, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (4): LeakyReLU(negative_slope=0.01)\n",
      "    (5): Dropout(p=0.4, inplace=False)\n",
      "    (6): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Dropout(p=0.4, inplace=False)\n",
      "    (9): Linear(in_features=32, out_features=2, bias=True)\n",
      "    (10): LeakyReLU(negative_slope=0.01)\n",
      "    (11): Dropout(p=0.4, inplace=False)\n",
      "  )\n",
      ")\n",
      "                                                                                                    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-4 (_monitor):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ekabore/anaconda3/lib/python3.11/threading.py\", line 1038, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/ekabore/Documents/personal/school/H2024/IFT6759/earthquake/env/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 761, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/Users/ekabore/anaconda3/lib/python3.11/threading.py\", line 975, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/ekabore/Documents/personal/school/H2024/IFT6759/earthquake/utils/gpu_monitoring.py\", line 59, in _monitor\n",
      "    utilization = gpu_util()\n",
      "                  ^^^^^^^^^^\n",
      "  File \"/Users/ekabore/Documents/personal/school/H2024/IFT6759/earthquake/utils/gpu_monitoring.py\", line 14, in gpu_util\n",
      "    result = subprocess.run(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ekabore/anaconda3/lib/python3.11/subprocess.py\", line 548, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ekabore/anaconda3/lib/python3.11/subprocess.py\", line 1024, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"/Users/ekabore/anaconda3/lib/python3.11/subprocess.py\", line 1791, in _execute_child\n",
      "    and os.path.dirname(executable)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<frozen posixpath>\", line 152, in dirname\n",
      "TypeError: expected str, bytes or os.PathLike object, not NoneType\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Train error: 0.5856 Train acc: 74.9% - Val error: 0.3760 Val acc: 90.9%                                                \n",
      "Epoch 2/20 - Train error: 0.5376 Train acc: 75.8% - Val error: 0.3991 Val acc: 90.9%                                                                                                                    \n",
      "Epoch 3/20 - Train error: 1.2645 Train acc: 71.4% - Val error: 3.3267 Val acc: 90.9%                                                                                                                    \n",
      "Epoch 4/20 - Train error: 321.0224 Train acc: 59.4% - Val error: 5.3836 Val acc: 90.9%                                                                                                                  \n",
      "Epoch 5/20 - Train error: 589.9654 Train acc: 59.1% - Val error: 0.7327 Val acc: 90.9%                                                                                                                  \n",
      "Epoch 6/20 - Train error: 101.6658 Train acc: 57.5% - Val error: 1.5895 Val acc: 90.9%                                                                                                                  \n",
      "Epoch 7/20 - Train error: 33.4380 Train acc: 59.4% - Val error: 1.3876 Val acc: 90.9%                                                                                                                   \n",
      "Epoch 8/20 - Train error: 70.6278 Train acc: 56.9% - Val error: 2.0390 Val acc: 90.9%                                                                                                                   \n",
      "Epoch 9/20 - Train error: 15.7703 Train acc: 59.6% - Val error: 1.5677 Val acc: 90.9%                                                                                                                   \n",
      "Epoch 10/20 - Train error: 14.2575 Train acc: 59.8% - Val error: 1.3026 Val acc: 90.9%                                                                                                                  \n",
      "Epoch 11/20 - Train error: 13.3868 Train acc: 59.3% - Val error: 0.8758 Val acc: 90.9%                                                                                                                  \n",
      "Epoch 12/20 - Train error: 5.0998 Train acc: 59.6% - Val error: 0.8029 Val acc: 90.9%                                                                                                                   \n",
      "Epoch 13/20 - Train error: 5.1197 Train acc: 60.4% - Val error: 0.6735 Val acc: 90.9%                                                                                                                   \n",
      "Epoch 14/20 - Train error: 2.9078 Train acc: 61.8% - Val error: 0.6494 Val acc: 90.9%                                                                                                                   \n",
      "Epoch 15/20 - Train error: 3.4879 Train acc: 62.0% - Val error: 0.8716 Val acc: 90.9%                                                                                                                   \n",
      "Epoch 16/20 - Train error: 5.5823 Train acc: 61.6% - Val error: 0.5945 Val acc: 90.9%                                                                                                                   \n",
      "Epoch 17/20 - Train error: 6.0840 Train acc: 62.9% - Val error: 0.5268 Val acc: 90.9%                                                                                                                   \n",
      "Epoch 18/20 - Train error: 4.3046 Train acc: 62.6% - Val error: 1.0256 Val acc: 90.9%                                                                                                                   \n",
      "Epoch 19/20 - Train error: 68.1702 Train acc: 58.9% - Val error: 2.7861 Val acc: 90.9%                                                                                                                  \n",
      "Epoch 20/20 - Train error: 81.3414 Train acc: 60.2% - Val error: 1.4851 Val acc: 90.9%                                                                                                                  \n"
     ]
    }
   ],
   "source": [
    "model = CNN(\n",
    "    input_channels=3,\n",
    "    conv_channels= [\n",
    "        8, 16, 32, 64, 128\n",
    "    ], kernel_sizes=[\n",
    "        11, 9, 7, 5, 3\n",
    "    ], mlp_layers=[\n",
    "        128, 64, 32, 2\n",
    "    ],\n",
    "    dropout=0.4\n",
    ")\n",
    "\n",
    "temp_dir = \"temp\"\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "e, a, model_path, monitor = train_detection_only(train_dataset, val_dataset, model, loss, correct, batch_size=64, epochs=20, temp_dir=temp_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6db000f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': [0.5856248837749974, 0.5376471167518979, 1.2644783732842426, 321.022350099622, 589.9654267531674, 101.66575787343136, 33.43796490406503, 70.62776178243209, 15.770300658178979, 14.25751043136428, 13.386756981311201, 5.099807813054039, 5.119657426058841, 2.907810773573765, 3.4878672402934963, 5.582325924821451, 6.083967663601142, 4.304614238998517, 68.17021813816359, 81.34136093676496], 'val': [0.37602370646264816, 0.39905362990167403, 3.326728890132573, 5.383646172781785, 0.732723188896974, 1.5895029654105504, 1.3876266380151112, 2.039011574453778, 1.567725055747562, 1.30262531257338, 0.8758188999361463, 0.8028628337714407, 0.6734844330284331, 0.6494057319230504, 0.8715837953819169, 0.5944798299007945, 0.5268197076188194, 1.025642178952694, 2.786103513505724, 1.4850917466812663]}\n",
      "{'train': [74.90909090909092, 75.80748663101605, 71.42245989304813, 59.401069518716575, 59.11229946524064, 57.50802139037433, 59.44385026737968, 56.855614973262036, 59.647058823529406, 59.77540106951872, 59.27272727272728, 59.604278074866315, 60.35294117647059, 61.786096256684495, 61.9572192513369, 61.55080213903743, 62.87700534759358, 62.62032085561498, 58.87700534759358, 60.19251336898396], 'val': [90.9090909090909, 90.9090909090909, 90.9090909090909, 90.9090909090909, 90.9090909090909, 90.9090909090909, 90.9090909090909, 90.9090909090909, 90.9090909090909, 90.9090909090909, 90.9090909090909, 90.9090909090909, 90.9090909090909, 90.9090909090909, 90.9090909090909, 90.9090909090909, 90.9090909090909, 90.9090909090909, 90.9090909090909, 90.9090909090909]}\n"
     ]
    }
   ],
   "source": [
    "print(e)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3852c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "figures_output = \"output/figures\"\n",
    "final_output_dir=\"output\"\n",
    "\n",
    "plot_error_and_accuracy(e, a, figures_output)\n",
    "monitor.save_plots(figures_output)\n",
    "shutil.copy(model_path, final_output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
