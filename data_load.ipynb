{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9638787b-eb90-44d3-8c50-1427bd34f858",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from __future__ import print_function\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed07198f-c559-4a08-a675-fb4e28a92eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############    Les trois canaux concaténés en ligne ################################### \n",
    "\n",
    "\n",
    "\n",
    "def load_dataset(filename):\n",
    "    with h5py.File(filename, 'r') as f:\n",
    "        if 'data' in f:\n",
    "            data = f['data']\n",
    "            if isinstance(data, h5py.Dataset):\n",
    "                print(\"Type du dataset 'data':\", type(data))\n",
    "                print(\"Forme du dataset 'data':\", data.shape)\n",
    "                data_matrix = data[:]  \n",
    "                print(\"Matrice de données:\", data_matrix)\n",
    "                return data_matrix\n",
    "            elif isinstance(data, h5py.Group):\n",
    "            \n",
    "                hdf5_data = []\n",
    "                trace_names = []\n",
    "                for key in data.keys():\n",
    "                    dataset = data[key]\n",
    "                    if isinstance(dataset, h5py.Dataset):\n",
    "                        # Vérifier la forme des données extraites\n",
    "                        if dataset.ndim == 2:\n",
    "                            # Concaténer les trois canaux pour chaque signal\n",
    "                            concatenated_data = np.concatenate([np.atleast_2d(dataset[i, :]) for i in range(3)], axis=1)\n",
    "                            hdf5_data.append(concatenated_data)\n",
    "                            trace_names.append(key)\n",
    "                        else:\n",
    "                            print(f\"Les données pour la clé '{key}' ne sont pas sous forme de tableau 2D.\")\n",
    "                if hdf5_data:\n",
    "                    stacked_data_matrix = np.vstack(hdf5_data)\n",
    "                    df_hdf5 = pd.DataFrame(stacked_data_matrix)\n",
    "                    df_hdf5['trace_name'] = trace_names\n",
    "                    return  df_hdf5\n",
    "\n",
    "\n",
    "# Utilisation de la fonction pour charger et convertir le dataset à partir du fichier HDF5\n",
    "filename1 = 'Instance_noise_1k.hdf5'\n",
    "df_noise = load_dataset(filename1)\n",
    "# Ajouter une colonne 'source_type' avec des valeurs constantes de 1\n",
    "df_noise['source_type'] = 1\n",
    "\n",
    "\n",
    "filename2 = 'Instance_events_counts_10k.hdf5'\n",
    "df_earth = load_dataset(filename2)\n",
    "len (df_earth)\n",
    "df_earth['source_type'] = 0\n",
    "print (df_earth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fcbbb8-7b2c-458f-a85d-9933446f0329",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "############################    récuperation des données dans les csv \n",
    "\n",
    "\n",
    "# Spécifier le chemin complet vers le fichier CSV\n",
    "csv_file1 = 'metadata_Instance_noise_1k.csv'\n",
    "# Lecture du fichier CSV contenant les informations sur la base de données des signaux\n",
    "noise_csv = pd.read_csv(csv_file1)\n",
    "\n",
    "# Spécifier le chemin complet vers le fichier CSV\n",
    "csv_file2 = 'metadata_Instance_events_10k.csv'\n",
    "# Lecture du fichier CSV contenant les informations sur la base de données des signaux\n",
    "earth_csv = pd.read_csv(csv_file2)\n",
    "print(earth_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95077e31-f845-4cf6-ba8e-b448ab281524",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e6e74780-2b0b-4faa-852e-50fd6f3e453c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['source_id', 'station_network_code', 'station_code',\n",
      "       'station_location_code', 'station_channels', 'station_latitude_deg',\n",
      "       'station_longitude_deg', 'station_elevation_m', 'station_vs_30_mps',\n",
      "       'station_vs_30_detail', 'trace_start_time', 'trace_dt_s', 'trace_npts',\n",
      "       'trace_E_median_counts', 'trace_N_median_counts',\n",
      "       'trace_Z_median_counts', 'trace_E_mean_counts', 'trace_N_mean_counts',\n",
      "       'trace_Z_mean_counts', 'trace_E_min_counts', 'trace_N_min_counts',\n",
      "       'trace_Z_min_counts', 'trace_E_max_counts', 'trace_N_max_counts',\n",
      "       'trace_Z_max_counts', 'trace_E_rms_counts', 'trace_N_rms_counts',\n",
      "       'trace_Z_rms_counts', 'trace_E_lower_quartile_counts',\n",
      "       'trace_N_lower_quartile_counts', 'trace_Z_lower_quartile_counts',\n",
      "       'trace_E_upper_quartile_counts', 'trace_N_upper_quartile_counts',\n",
      "       'trace_Z_upper_quartile_counts', 'trace_E_spikes', 'trace_N_spikes',\n",
      "       'trace_Z_spikes', 'trace_name', 'trace_GPD_P_number',\n",
      "       'trace_GPD_S_number', 'trace_EQT_number_detections',\n",
      "       'trace_EQT_P_number', 'trace_EQT_S_number'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "################    récuperation des nom de colonnes en commun dans le csv de noise et earthquake\n",
    "\n",
    "def  common_columns(csv_file1,csv_file2):\n",
    "\n",
    "    earth_csv = pd.read_csv(csv_file1) \n",
    "    noise_csv = pd.read_csv(csv_file2)\n",
    "    # Obtenir les noms de colonnes en commun\n",
    "    common_columns = earth_csv.columns.intersection(noise_csv.columns)\n",
    "    # Afficher les noms de colonnes en commun\n",
    "    print(common_columns)\n",
    "\n",
    "# Spécifier le chemin complet vers le fichier CSV\n",
    "csv_file1 = 'metadata_Instance_events_10k.csv'\n",
    "# Spécifier le chemin complet vers le fichier CSV\n",
    "csv_file2 = 'metadata_Instance_noise_1k.csv'\n",
    "\n",
    "common_columns(csv_file1,csv_file2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74b38e2-2f96-4998-8ab4-fd4547727b79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
