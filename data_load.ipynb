{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9638787b-eb90-44d3-8c50-1427bd34f858",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from __future__ import print_function\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed07198f-c559-4a08-a675-fb4e28a92eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############    Les trois canaux concaténés en ligne ################################### \n",
    "\n",
    "\n",
    "\n",
    "def load_dataset(filename):\n",
    "    with h5py.File(filename, 'r') as f:\n",
    "        if 'data' in f:\n",
    "            data = f['data']\n",
    "            if isinstance(data, h5py.Dataset):\n",
    "                print(\"Type du dataset 'data':\", type(data))\n",
    "                print(\"Forme du dataset 'data':\", data.shape)\n",
    "                data_matrix = data[:]  \n",
    "                print(\"Matrice de données:\", data_matrix)\n",
    "                return data_matrix\n",
    "            elif isinstance(data, h5py.Group):\n",
    "            \n",
    "                hdf5_data = []\n",
    "                trace_names = []\n",
    "                for key in data.keys():\n",
    "                    dataset = data[key]\n",
    "                    if isinstance(dataset, h5py.Dataset):\n",
    "                        # Vérifier la forme des données extraites\n",
    "                        if dataset.ndim == 2:\n",
    "                            # Concaténer les trois canaux pour chaque signal\n",
    "                            concatenated_data = np.concatenate([np.atleast_2d(dataset[i, :]) for i in range(3)], axis=1)\n",
    "                            hdf5_data.append(concatenated_data)\n",
    "                            trace_names.append(key)\n",
    "                        else:\n",
    "                            print(f\"Les données pour la clé '{key}' ne sont pas sous forme de tableau 2D.\")\n",
    "                if hdf5_data:\n",
    "                    stacked_data_matrix = np.vstack(hdf5_data)\n",
    "                    df_hdf5 = pd.DataFrame(stacked_data_matrix)\n",
    "                    df_hdf5['trace_name'] = trace_names\n",
    "                    return  df_hdf5\n",
    "\n",
    "\n",
    "# Utilisation de la fonction pour charger et convertir le dataset à partir du fichier HDF5\n",
    "filename1 = 'Instance_noise_1k.hdf5'\n",
    "df_noise = load_dataset(filename1)\n",
    "# Ajouter une colonne 'source_type' avec des valeurs constantes de 1\n",
    "df_noise['source_type'] = 1\n",
    "\n",
    "\n",
    "filename2 = 'Instance_events_counts_10k.hdf5'\n",
    "df_earth = load_dataset(filename2)\n",
    "len (df_earth)\n",
    "df_earth['source_type'] = 0\n",
    "print (df_earth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "16fcbbb8-7b2c-458f-a85d-9933446f0329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      source_id station_network_code station_code  station_location_code  \\\n",
      "0      11030611                   IV         OFFI                    NaN   \n",
      "1      11030611                   IV         PIEI                    NaN   \n",
      "2      11030611                   IV         PIEI                    NaN   \n",
      "3      11030611                   IV         RM33                    NaN   \n",
      "4      11030611                   IV         RM33                    NaN   \n",
      "...         ...                  ...          ...                    ...   \n",
      "9995   11282341                   IV         MPAG                    NaN   \n",
      "9996   11282341                   MN          BLY                    NaN   \n",
      "9997   11282341                   MN          BLY                    NaN   \n",
      "9998   11282341                   MN          CUC                    NaN   \n",
      "9999   11282341                   MN          CUC                    NaN   \n",
      "\n",
      "     station_channels  station_latitude_deg  station_longitude_deg  \\\n",
      "0                  HH              42.93500               13.68570   \n",
      "1                  HH              43.53567               12.53500   \n",
      "2                  HN              43.53567               12.53500   \n",
      "3                  EH              42.50898               13.21452   \n",
      "4                  HN              42.50898               13.21452   \n",
      "...               ...                   ...                    ...   \n",
      "9995               EH              43.62920               12.75950   \n",
      "9996               HH              44.74880               17.18390   \n",
      "9997               HL              44.74880               17.18390   \n",
      "9998               HH              39.99380               15.81550   \n",
      "9999               HN              39.99380               15.81550   \n",
      "\n",
      "      station_elevation_m  station_vs_30_mps          station_vs_30_detail  \\\n",
      "0                     320         580.000000  Vs30 extracted from ShakeMap   \n",
      "1                     665         778.339556  Vs30 extracted from ShakeMap   \n",
      "2                     665         778.339556  Vs30 extracted from ShakeMap   \n",
      "3                    1097         721.279254  Vs30 extracted from ShakeMap   \n",
      "4                    1097         721.279254  Vs30 extracted from ShakeMap   \n",
      "...                   ...                ...                           ...   \n",
      "9995                  930         579.192516  Vs30 extracted from ShakeMap   \n",
      "9996                  256         765.271418  Vs30 extracted from ShakeMap   \n",
      "9997                  256         765.271418  Vs30 extracted from ShakeMap   \n",
      "9998                  637         807.440158  Vs30 extracted from ShakeMap   \n",
      "9999                  637         807.440158  Vs30 extracted from ShakeMap   \n",
      "\n",
      "      ... trace_sa10_cmps2  trace_sa30_cmps2            trace_name  \\\n",
      "0     ...         0.001040          0.000094  11030611.IV.OFFI..HH   \n",
      "1     ...         0.000053          0.000015  11030611.IV.PIEI..HH   \n",
      "2     ...         0.000239          0.000078  11030611.IV.PIEI..HN   \n",
      "3     ...         0.000244          0.000055  11030611.IV.RM33..EH   \n",
      "4     ...         0.000301          0.000075  11030611.IV.RM33..HN   \n",
      "...   ...              ...               ...                   ...   \n",
      "9995  ...         0.005525          0.000803  11282341.IV.MPAG..EH   \n",
      "9996  ...         0.026140          0.001762   11282341.MN.BLY..HH   \n",
      "9997  ...         0.025938          0.001781   11282341.MN.BLY..HL   \n",
      "9998  ...         0.002233          0.000331   11282341.MN.CUC..HH   \n",
      "9999  ...         0.002003          0.000345   11282341.MN.CUC..HN   \n",
      "\n",
      "      trace_GPD_P_number  trace_GPD_S_number  trace_EQT_number_detections  \\\n",
      "0                      1                   2                          1.0   \n",
      "1                      0                   2                          1.0   \n",
      "2                      2                   1                          NaN   \n",
      "3                      3                   7                          2.0   \n",
      "4                      1                   3                          NaN   \n",
      "...                  ...                 ...                          ...   \n",
      "9995                   1                   1                          0.0   \n",
      "9996                   1                   1                          1.0   \n",
      "9997                   1                   1                          NaN   \n",
      "9998                   1                   2                          1.0   \n",
      "9999                   0                   2                          NaN   \n",
      "\n",
      "      trace_EQT_P_number  trace_EQT_S_number  trace_deconvolved_units  \\\n",
      "0                    1.0                 1.0                      mps   \n",
      "1                    1.0                 1.0                      mps   \n",
      "2                    NaN                 NaN                     mps2   \n",
      "3                    2.0                 2.0                      mps   \n",
      "4                    NaN                 NaN                     mps2   \n",
      "...                  ...                 ...                      ...   \n",
      "9995                 0.0                 0.0                      mps   \n",
      "9996                 1.0                 1.0                      mps   \n",
      "9997                 NaN                 NaN                     mps2   \n",
      "9998                 1.0                 0.0                      mps   \n",
      "9999                 NaN                 NaN                     mps2   \n",
      "\n",
      "      source_type  \n",
      "0      earthquake  \n",
      "1      earthquake  \n",
      "2      earthquake  \n",
      "3      earthquake  \n",
      "4      earthquake  \n",
      "...           ...  \n",
      "9995   earthquake  \n",
      "9996   earthquake  \n",
      "9997   earthquake  \n",
      "9998   earthquake  \n",
      "9999   earthquake  \n",
      "\n",
      "[10000 rows x 115 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "############################    récuperation des données dans les csv \n",
    "\n",
    "\n",
    "# Spécifier le chemin complet vers le fichier CSV\n",
    "csv_file1 = 'metadata_Instance_noise_1k.csv'\n",
    "# Lecture du fichier CSV contenant les informations sur la base de données des signaux\n",
    "noise_csv = pd.read_csv(csv_file1)\n",
    "\n",
    "# Spécifier le chemin complet vers le fichier CSV\n",
    "csv_file2 = 'metadata_Instance_events_10k.csv'\n",
    "# Lecture du fichier CSV contenant les informations sur la base de données des signaux\n",
    "earth_csv = pd.read_csv(csv_file2)\n",
    "print(earth_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95077e31-f845-4cf6-ba8e-b448ab281524",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e6e74780-2b0b-4faa-852e-50fd6f3e453c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['source_id', 'station_network_code', 'station_code',\n",
      "       'station_location_code', 'station_channels', 'station_latitude_deg',\n",
      "       'station_longitude_deg', 'station_elevation_m', 'station_vs_30_mps',\n",
      "       'station_vs_30_detail', 'trace_start_time', 'trace_dt_s', 'trace_npts',\n",
      "       'trace_E_median_counts', 'trace_N_median_counts',\n",
      "       'trace_Z_median_counts', 'trace_E_mean_counts', 'trace_N_mean_counts',\n",
      "       'trace_Z_mean_counts', 'trace_E_min_counts', 'trace_N_min_counts',\n",
      "       'trace_Z_min_counts', 'trace_E_max_counts', 'trace_N_max_counts',\n",
      "       'trace_Z_max_counts', 'trace_E_rms_counts', 'trace_N_rms_counts',\n",
      "       'trace_Z_rms_counts', 'trace_E_lower_quartile_counts',\n",
      "       'trace_N_lower_quartile_counts', 'trace_Z_lower_quartile_counts',\n",
      "       'trace_E_upper_quartile_counts', 'trace_N_upper_quartile_counts',\n",
      "       'trace_Z_upper_quartile_counts', 'trace_E_spikes', 'trace_N_spikes',\n",
      "       'trace_Z_spikes', 'trace_name', 'trace_GPD_P_number',\n",
      "       'trace_GPD_S_number', 'trace_EQT_number_detections',\n",
      "       'trace_EQT_P_number', 'trace_EQT_S_number'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "################    récuperation des nom de colonnes en commun dans le csv de noise et earthquake\n",
    "\n",
    "def  common_columns(csv_file1,csv_file2):\n",
    "\n",
    "    earth_csv = pd.read_csv(csv_file1) \n",
    "    noise_csv = pd.read_csv(csv_file2)\n",
    "    # Obtenir les noms de colonnes en commun\n",
    "    common_columns = earth_csv.columns.intersection(noise_csv.columns)\n",
    "    # Afficher les noms de colonnes en commun\n",
    "    print(common_columns)\n",
    "\n",
    "# Spécifier le chemin complet vers le fichier CSV\n",
    "csv_file1 = 'metadata_Instance_events_10k.csv'\n",
    "# Spécifier le chemin complet vers le fichier CSV\n",
    "csv_file2 = 'metadata_Instance_noise_1k.csv'\n",
    "\n",
    "common_columns(csv_file1,csv_file2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74b38e2-2f96-4998-8ab4-fd4547727b79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
