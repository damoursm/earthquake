{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7202cd6-a6d6-474a-a3ca-11b80f09ff0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "from __future__ import print_function\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# PyTorch Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfddf27e-57ce-46a8-a2ba-3fec998f696f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 3, 12000)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import math\n",
    "\n",
    "def load_dataset(filename):\n",
    "    with h5py.File(filename, 'r') as f:\n",
    "        if 'data' in f:\n",
    "            data = f['data']\n",
    "            if isinstance(data, h5py.Dataset):\n",
    "                print(\"Type du dataset 'data':\", type(data))\n",
    "                print(\"Forme du dataset 'data':\", data.shape)\n",
    "                data_matrix = data[:]  \n",
    "                print(\"Matrice de données:\", data_matrix)\n",
    "                return data_matrix\n",
    "            elif isinstance(data, h5py.Group):\n",
    "                data_list = []\n",
    "                trace_names = []\n",
    "                for key in data.keys():\n",
    "                    dataset = data[key]\n",
    "                    if isinstance(dataset, h5py.Dataset):\n",
    "                        # Vérifier la forme des données extraites\n",
    "                        if dataset.ndim == 2:\n",
    "                            data_array = np.zeros((1, 3, dataset.shape[1]), dtype=np.float32)  # Créer une matrice vide de quatre dimensions\n",
    "                            for i in range(3):  # Boucle à travers les canaux\n",
    "                                data_array[:, i, :] = dataset[i, :].reshape(1,  dataset.shape[1])  # Réorganiser les données dans le bon ordre\n",
    "                            data_list.append(data_array)\n",
    "                            #trace_names.append(key)\n",
    "                            # Concaténer les noms de clés avec un tableau de zéros\n",
    "                            trace_names = np.concatenate([np.array([key] ).reshape(-1, 1) for key in data.keys()])\n",
    "                            # Remodeler en un vecteur colonne\n",
    "                            trace_names=trace_names.reshape(-1, 1)                          \n",
    "                            #trace_names['trace_names']=trace_names\n",
    "                        \n",
    "                        else:\n",
    "                            print(f\"Les données pour la clé '{key}' ne sont pas sous forme de tableau 2D.\")\n",
    "                if data_list:\n",
    "                    stacked_data_matrix = np.vstack(data_list)  # Empiler les matrices de données dans une seule matrice\n",
    "                    return stacked_data_matrix, trace_names\n",
    "                    \n",
    "# Utilisation de la foncti, our charger et convertir le dataset à partir du fichier HDF5\n",
    "filename1 = 'Instance_noise_1k.hdf5'\n",
    "noise_matrix, trace_names_noise = load_dataset(filename1)\n",
    "\n",
    "filename2 = 'Instance_events_counts_10k.hdf5'\n",
    "events_matrix, trace_names_events = load_dataset(filename2)\n",
    "\n",
    "print (events_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3557d709-f44f-4372-a2c4-2549e2487a51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d095f5d-8836-4eac-aab5-44ae2858f788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 3, 12000)\n",
      "(1000, 3, 12000)\n",
      "[['11030611.IV.OFFI..HH' '1']\n",
      " ['11030611.IV.PIEI..HH' '1']\n",
      " ['11030611.IV.PIEI..HN' '1']\n",
      " ...\n",
      " ['11052061.IV.TERO..HH' '1']\n",
      " ['11052061.IV.VCEL..EH' '1']\n",
      " ['11052061.XO.AM05..EH' '1']]\n"
     ]
    }
   ],
   "source": [
    "# Création d'une colonne 'source_type' avec des valeurs constantes de 1\n",
    "source_type_n = np.zeros((len(trace_names_noise), 1), dtype=int)\n",
    "\n",
    "# Création d'une colonne 'source_type' avec des valeurs constantes de 1\n",
    "source_type_e = np.ones((len(trace_names_events), 1), dtype=int)\n",
    "\n",
    "# Concaténation de la colonne 'source_type' avec trace_names_noise\n",
    "trace_names_n = np.concatenate((trace_names_noise, source_type_n), axis=1)\n",
    "trace_names_e = np.concatenate((trace_names_events, source_type_e), axis=1)\n",
    "\n",
    "\n",
    "trace_names_e = np.concatenate((trace_names_events[:len(trace_names_noise)], source_type_e[:len(trace_names_noise)]), axis=1)\n",
    "#Prendre autant d'echantillon evant que de noise\n",
    "events_matrix= events_matrix[:len(noise_matrix),:,:]\n",
    "print(events_matrix.shape)\n",
    "print(noise_matrix.shape)\n",
    "\n",
    "# Vérification de la forme de la matrice résultante\n",
    "print(trace_names_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd52a10f-7346-42d6-881d-a6a7e0fb110e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 3.7057e-03,  3.7057e-03,  3.7057e-03,  ...,  3.7057e-03,\n",
      "           8.6438e-01, -8.5697e-01],\n",
      "         [ 3.7057e-03,  3.7057e-03,  3.7057e-03,  ...,  3.7057e-03,\n",
      "           3.7057e-03, -8.5697e-01],\n",
      "         [ 3.7057e-03,  3.7057e-03,  3.7057e-03,  ...,  3.7057e-03,\n",
      "           3.7057e-03, -8.5697e-01]],\n",
      "\n",
      "        [[ 2.2457e-02, -4.1898e-01, -2.8410e-01,  ...,  1.5938e-01,\n",
      "           4.2894e-02,  1.7778e-01],\n",
      "         [ 6.1287e-02,  3.8624e-01,  8.5811e-02,  ..., -2.6979e-01,\n",
      "          -2.7592e-01, -4.7825e-01],\n",
      "         [-1.2265e-01, -2.4548e-02, -6.1335e-02,  ..., -2.1666e-01,\n",
      "          -1.6965e-01, -1.0017e-01]],\n",
      "\n",
      "        [[ 1.1620e+00,  1.1403e+00,  1.0969e+00,  ...,  9.2311e-01,\n",
      "           1.1186e+00,  1.2163e+00],\n",
      "         [ 2.0635e+00,  2.0635e+00,  2.1612e+00,  ...,  2.5413e+00,\n",
      "           2.3133e+00,  1.9223e+00],\n",
      "         [-7.8201e-01, -8.6889e-01, -9.0147e-01,  ..., -2.0093e+00,\n",
      "          -1.9332e+00, -1.9984e+00]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.1839e+00, -3.1468e+00, -2.0641e+00,  ...,  1.0120e-01,\n",
      "           1.1839e+00,  1.0120e-01],\n",
      "         [ 1.0120e-01, -9.8147e-01, -9.8147e-01,  ...,  1.0120e-01,\n",
      "           1.0120e-01, -2.0641e+00],\n",
      "         [ 1.1839e+00,  1.0120e-01,  1.0120e-01,  ...,  1.0120e-01,\n",
      "           1.0120e-01, -2.0641e+00]],\n",
      "\n",
      "        [[-1.4675e+00, -4.2351e-02,  2.1623e+00,  ...,  1.1434e+00,\n",
      "          -2.1055e+00,  2.1931e-01],\n",
      "         [-2.3362e-01, -4.6174e-01, -5.3270e-01,  ...,  7.3713e-01,\n",
      "          -7.2797e-02,  5.5056e-02],\n",
      "         [ 5.9875e-01,  7.4402e-03,  8.6419e-02,  ...,  9.7865e-02,\n",
      "           2.2011e-01,  3.4636e-01]],\n",
      "\n",
      "        [[-6.1108e-01, -2.0746e-01, -6.4213e-01,  ...,  8.9991e-01,\n",
      "           3.8245e-01, -2.2816e-01],\n",
      "         [ 3.1000e-01,  1.3242e+00,  7.3432e-01,  ..., -2.8051e+00,\n",
      "          -3.4778e+00, -4.3058e+00],\n",
      "         [ 6.9293e-01,  5.4804e-01,  4.9629e-01,  ...,  4.1350e-01,\n",
      "           2.1686e-01,  2.1686e-01]]])\n"
     ]
    }
   ],
   "source": [
    "# Préparation des données d'entrainement, validation et de test\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset\n",
    "import torch \n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# Diviser les données de bruit en ensemble d'entraînement et ensemble de test\n",
    "df_noise_train1, df_noise_test = train_test_split(noise_matrix, test_size=0.20, random_state=42)\n",
    "df_noise_train, df_noise_validation = train_test_split(df_noise_train1, test_size=0.20, random_state=42)\n",
    "\n",
    "trace_noise1, trace_noise_test =train_test_split(trace_names_n, test_size=0.20, random_state=42)\n",
    "trace_noise_train, trace_noise_val = train_test_split(trace_noise1, test_size=0.20, random_state=42)\n",
    "\n",
    "trace_events1, trace_events_test =train_test_split(trace_names_e, test_size=0.20, random_state=42)\n",
    "trace_events_train, trace_events_val = train_test_split(trace_events1, test_size=0.20, random_state=42)\n",
    "\n",
    "\n",
    "df_train1, df_test = train_test_split(events_matrix, test_size=0.20, random_state=42)\n",
    "df_train, df_validation = train_test_split(df_train1, test_size=0.20, random_state=42)\n",
    "\n",
    "\n",
    "# Concaténer df_noise_train avec df_train\n",
    "df_train_concat = np.concatenate([df_train, df_noise_train], axis=0)\n",
    "trace_train = np.concatenate([trace_events_train, trace_noise_train], axis=0)\n",
    "trace_train = trace_train[:,1].astype(int)\n",
    "\n",
    "# Concaténer df_noise_test avec df_test\n",
    "df_test_concat = np.concatenate([df_test, df_noise_test], axis=0)\n",
    "trace_test= np.concatenate([trace_events_test, trace_noise_test], axis=0)\n",
    "# Convertir les étiquettes en entiers\n",
    "trace_test = trace_test[:,1].astype(int)\n",
    "\n",
    "#print (df_test_concat)\n",
    "# Concaténer df_noise_validation avec df_validation\n",
    "df_val_concat = np.concatenate([df_validation, df_noise_validation], axis=0)\n",
    "trace_val= np.concatenate([trace_events_val, trace_noise_val], axis=0)\n",
    "trace_val = trace_val[:,1].astype(int)\n",
    "\"\"\"\n",
    "# Normaliser toute la donnée\n",
    "mean_t = np.mean(df_train_concat)\n",
    "std_t = np.std(df_train_concat)\n",
    "norm_data_t = (df_train_concat - mean_t) / std_t\n",
    "\n",
    "#print(norm_data_t)\n",
    "\n",
    "mean_v = np.mean(df_val_concat)\n",
    "std_v = np.std(df_val_concat)\n",
    "norm_data_v = (df_val_concat - mean_v) / std_v\n",
    "\n",
    "mean_ts = np.mean(df_test_concat)\n",
    "std_ts = np.std(df_test_concat)\n",
    "norm_data_ts = (df_test_concat - mean_ts) / std_ts\n",
    "\"\"\"\n",
    "# Normaliser toute la donnée\n",
    "def normalize_data(data):\n",
    "    mean = np.mean(data, axis=(1, 2), keepdims=True)  # Calculer la moyenne de chaque échantillon\n",
    "    std = np.std(data, axis=(1, 2), keepdims=True)    # Calculer l'écart-type de chaque échantillon\n",
    "    norm_data = (data - mean) / std                  # Normaliser chaque échantillon\n",
    "    return norm_data\n",
    "\n",
    "# Appliquer la normalisation sur les données d'entraînement, de validation et de test\n",
    "norm_data_train = normalize_data(df_train_concat)\n",
    "norm_data_val = normalize_data(df_val_concat)\n",
    "norm_data_test = normalize_data(df_test_concat)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# Appliquer l'aplatissement sur les données normalisées\n",
    "fla_data_train = norm_data_train.view(norm_data_train.size(0), -1)\n",
    "fla_data_val = norm_data_val.view(norm_data_val.size(0), -1)\n",
    "fla_data_test = norm_data_test.view(norm_data_test.size(0), -1)\n",
    "\"\"\"\n",
    "norm_data_t = norm_data_train \n",
    "norm_data_v = norm_data_val \n",
    "norm_data_ts = norm_data_test\n",
    "\n",
    "\n",
    "#print(trace_train.shape)\n",
    "\n",
    "#print(norm_data_t.shape)\n",
    "\n",
    "# Convert your numpy arrays to PyTorch tensors\n",
    "x_train_tensor = torch.tensor(norm_data_t, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(trace_train, dtype=torch.long)  # long for CrossEntropyLoss\n",
    "\n",
    "x_test_tensor = torch.tensor(norm_data_ts, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(trace_test, dtype=torch.long)\n",
    "\n",
    "x_val_tensor = torch.tensor(norm_data_v, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(trace_val, dtype=torch.long)\n",
    "\n",
    "\"\"\"\n",
    "####################\n",
    "# Appliquer l'aplatissement sur les données normalisées\n",
    "x_train_tensor= x_train_tensor.view(x_train_tensor.size(0), -1)\n",
    "x_val_tensor = x_val_tensor .view(x_val_tensor .size(0), -1)\n",
    "x_test_tensor = x_test_tensor.view(x_test_tensor.size(0), -1)\n",
    "#########################\n",
    "\"\"\"\n",
    "y_train_tensor = y_train_tensor.unsqueeze(1)\n",
    "y_val_tensor = y_val_tensor.unsqueeze(1)\n",
    "y_test_tensor = y_test_tensor.unsqueeze(1)\n",
    "\n",
    "print(x_val_tensor)\n",
    "#print( y_val_tensor)\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
    "val_dataset = TensorDataset(x_val_tensor, y_val_tensor)\n",
    "\n",
    "\n",
    "# Créer les DataLoaders pour l'entraînement, la validation et le test\n",
    "batch_size =64  # Vous pouvez changer cette valeur selon vos besoins\n",
    "\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "721196e7-9c76-4dd7-b336-81b5db6e6dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configuration 1 :\n",
    "import numpy as np\n",
    "import random\n",
    "# PyTorch Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class cnnModel(nn.Module):\n",
    "    def __init__(self, time_periods, n_sensors, n_classes):\n",
    "        super(cnnModel, self).__init__()\n",
    "        self.time_periods = time_periods\n",
    "        self.n_sensors = n_sensors\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv1d(n_sensors, 100, kernel_size=5)\n",
    "        self.conv2 = nn.Conv1d(100, 160, kernel_size=5)\n",
    "        \n",
    "        # Pooling and dropout\n",
    "        self.pool = nn.MaxPool1d(kernel_size=3)\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(160, n_classes)\n",
    "\n",
    "        # Initialiser les poids avec Xavier initialization\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d) or isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.n_sensors, self.time_periods)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = self.dropout(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return torch.sigmoid(x)\n",
    "        \n",
    "        \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406588a5-1d6a-42db-8840-b06157b846cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e15ee61-c5e1-4b43-8e8f-835459c287c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_confusion_matrix(validations, predictions, title=None):\n",
    "    matrix = metrics.confusion_matrix(validations, predictions)\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(matrix,\n",
    "                cmap='coolwarm',\n",
    "                linecolor='white',\n",
    "                linewidths=1,\n",
    "                xticklabels=LABELS,\n",
    "                yticklabels=LABELS,\n",
    "                annot=True,\n",
    "                fmt='d')\n",
    "    if title: plt.title(title)\n",
    "    else: plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b27028e7-7753-4b7c-b1cf-c2e3c71e619d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_performance(train_losses, val_losses, train_accs, val_accs, params=None, save_filename=None ):\n",
    "    epochs = len(train_losses)  # Obtenez le nombre d'époques réellement effectuées\n",
    "    #plt.title('Loss and Accuracy over epochs\\n' + f'Params: {params}')\n",
    "\n",
    "    #plt.figure(figsize=(15, 5))\n",
    "\n",
    "    # Plot losses\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, epochs + 1), train_losses, label='Train Loss', color='red')\n",
    "    plt.plot(range(1, epochs + 1), val_losses, label='Validation Loss', color='blue')\n",
    "\n",
    "   # plt.title('Loss over epochs\\n' + f'Params: {params}')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot accuracies\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(1, epochs + 1), train_accs, label='Train Accuracy', color='red')\n",
    "    plt.plot(range(1, epochs + 1), val_accs, label='Validation Accuracy', color='blue')\n",
    "\n",
    "   # plt.title('Accuracy over epochs\\n' + f'Params: {params}')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Sauvegarder la figure dans un fichier image\n",
    "    if save_filename:\n",
    "        param_str = '_'.join([f'{key}_{value}' if not isinstance(value, types.FunctionType) else f'{key}_{value.__name__}' for key, value in params.items()])\n",
    "        filename = f'{save_filename}_{param_str}.png'\n",
    "        plt.savefig(filename)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    # Sauvegarder la figure dans un fichier image\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5091dd50-99d8-45a5-8cea-347e4efd6d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnnModel(\n",
      "  (conv1): Conv1d(3, 100, kernel_size=(5,), stride=(1,))\n",
      "  (conv2): Conv1d(100, 160, kernel_size=(5,), stride=(1,))\n",
      "  (pool): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "  (adaptive_pool): AdaptiveAvgPool1d(output_size=1)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (fc): Linear(in_features=160, out_features=2, bias=True)\n",
      ")\n",
      "Training the model...\n",
      "Epoch [1/10], Train Loss: 0.7877, Train Accuracy: 0.5055\n",
      "Epoch [1/10], Validation Loss: 0.8079, Validation Accuracy: 0.5062\n",
      "Epoch [2/10], Train Loss: 0.7782, Train Accuracy: 0.5266\n",
      "Epoch [2/10], Validation Loss: 0.8079, Validation Accuracy: 0.5062\n",
      "Epoch [3/10], Train Loss: 0.7806, Train Accuracy: 0.5172\n",
      "Epoch [3/10], Validation Loss: 0.8079, Validation Accuracy: 0.5062\n",
      "Epoch [4/10], Train Loss: 0.7882, Train Accuracy: 0.5023\n",
      "Epoch [4/10], Validation Loss: 0.8079, Validation Accuracy: 0.5062\n",
      "Epoch [5/10], Train Loss: 0.7776, Train Accuracy: 0.5227\n",
      "Epoch [5/10], Validation Loss: 0.8079, Validation Accuracy: 0.5062\n",
      "Epoch [6/10], Train Loss: 0.7724, Train Accuracy: 0.5305\n",
      "Epoch [6/10], Validation Loss: 0.8079, Validation Accuracy: 0.5062\n",
      "Epoch [7/10], Train Loss: 0.7676, Train Accuracy: 0.5383\n",
      "Epoch [7/10], Validation Loss: 0.8079, Validation Accuracy: 0.5062\n",
      "Epoch [8/10], Train Loss: 0.7791, Train Accuracy: 0.5188\n",
      "Epoch [8/10], Validation Loss: 0.8079, Validation Accuracy: 0.5062\n",
      "Epoch [9/10], Train Loss: 0.7760, Train Accuracy: 0.5250\n",
      "Epoch [9/10], Validation Loss: 0.8079, Validation Accuracy: 0.5062\n",
      "Epoch [10/10], Train Loss: 0.7731, Train Accuracy: 0.5352\n",
      "Epoch [10/10], Validation Loss: 0.8079, Validation Accuracy: 0.5062\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.optim as optim\n",
    "from torch.optim import AdamW\n",
    "from torch.optim import Adam\n",
    "#Check if CUDA is available (for GPU usage)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "EPOCHS =10 # Vous pouvez ajuster le nombre d'époques selon vos besoins\n",
    "n_sensors = 3\n",
    "n_classes = 2\n",
    "TIME_PERIODS =12000\n",
    "\n",
    "#input_shape = (batch_size, 3, 12000)\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Initialize lists to store losses and accuracies\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "\n",
    "def train_cnn(model_cnn, device, num_epochs, my_optimizer):\n",
    "    # Initialize lists to store losses and accuracies\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training mode\n",
    "        model_cnn.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            #print ( len(labels))\n",
    "            #print(inputs.shape)\n",
    "            \n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Conversion des étiquettes en tenseurs PyTorch et déclaration de l'exigence de gradients\n",
    "            labels = torch.tensor(labels, dtype=torch.float, device=device, requires_grad=True)\n",
    "           \n",
    "            #print(labels)\n",
    "            my_optimizer.zero_grad()\n",
    "            outputs = model_cnn(inputs)\n",
    "            \n",
    "            #print(outputs.shape)\n",
    "\n",
    "            # Calcul des classes prédites\n",
    "            predicted_class = torch.argmax(outputs, dim=1)\n",
    "            # Réduction de dimension des classes prédites à (64, 1)\n",
    "            predicted_class = predicted_class.view(-1, 1)\n",
    "            \n",
    "            predicted_cl = predicted_class.float()\n",
    "            #print(predicted_cl)\n",
    "           \n",
    "            # Calculer la perte\n",
    "            loss = criterion(predicted_cl, labels)\n",
    "            loss.backward()\n",
    "            my_optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "            correct_train += (predicted_cl == labels).sum().item()\n",
    "            #print(correct_train)\n",
    "            total_train += labels.size(0)\n",
    "            #print (total_train)\n",
    "            #print (train_loader)\n",
    "       \n",
    "        # Update the learning rate with the scheduler\n",
    "     \n",
    "        epoch_train_loss = running_loss / len(train_loader)\n",
    "        epoch_train_acc = correct_train / total_train\n",
    "        \n",
    "        train_losses.append(epoch_train_loss)\n",
    "        train_accs.append(epoch_train_acc)\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {epoch_train_loss:.4f}, Train Accuracy: {epoch_train_acc:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "        # Validation mode\n",
    "        model_cnn.eval()\n",
    "        val_loss = 0.0\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs_val = model_cnn(inputs)\n",
    "                #print(outputs)\n",
    "                \n",
    "                # Assurez-vous que les étiquettes (labels) sont également des tenseurs de type float\n",
    "                labels = labels.float()\n",
    "                predicted_class_val = torch.argmax(outputs_val, dim=1)\n",
    "                \n",
    "                # Réduction de dimension des classes prédites à (64, 1)\n",
    "                predicted_class_val = predicted_class_val.view(-1, 1)\n",
    "            \n",
    "                predicted_cl_val = predicted_class_val.float()\n",
    "                #print(predicted_cl_val)\n",
    "\n",
    "                # Calculer la perte\n",
    "                loss = criterion(predicted_cl_val, labels)\n",
    "                val_loss += loss.item()\n",
    "               \n",
    "                # Calculer l'exactitude\n",
    "              \n",
    "                correct_val += (predicted_cl_val == labels).sum().item()\n",
    "                total_val += labels.size(0)\n",
    "\n",
    "            epoch_val_loss = val_loss / len(val_loader)\n",
    "            epoch_val_acc = correct_val / total_val\n",
    "        \n",
    "            val_losses.append(epoch_val_loss)\n",
    "            val_accs.append(epoch_val_acc)\n",
    "\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Validation Loss: {epoch_val_loss:.4f}, Validation Accuracy: {epoch_val_acc:.4f}\")\n",
    "\n",
    "    return train_losses, val_losses, train_accs, val_accs\n",
    " \n",
    "def test_cnn(model_cnn, device):\n",
    "    model_cnn.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs_test = model_cnn(inputs)\n",
    "            predicted_class_test = torch.argmax(outputs_test, dim=1)\n",
    "                \n",
    "            # Réduction de dimension des classes prédites à (64, 1)\n",
    "            predicted_class_test = predicted_class_test.view(-1, 1)\n",
    "            \n",
    "            predicted_cl_test = predicted_class_test.float()\n",
    "                       \n",
    "            # Assurez-vous que les étiquettes (labels) sont également des tenseurs de type float\n",
    "            labels = labels.float()\n",
    "            #print(predicated)   \n",
    "            # Calculer la perte en utilisant les prédictions binaires et les étiquettes\n",
    "            loss = criterion(predicted_cl_test, labels)\n",
    "            test_loss += loss.item()\n",
    "            # Calculer l'exactitude\n",
    "            correct_test += (predicted_cl_test== labels).sum().item()\n",
    "            total_test += labels.size(0)\n",
    "\n",
    "        # Calculer la perte moyenne et l'exactitude\n",
    "        test_loss /= len(test_loader)\n",
    "        test_acc = correct_test / total_test\n",
    "\n",
    "        print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
    "        LABELS= [0, 1]\n",
    "        show_confusion_matrix(labels, predicted)\n",
    "        \n",
    "    return test_loss, test_acc\n",
    "    \n",
    "# Assuming TIME_PERIODS, n_sensors, and n_classes are defined\n",
    "model_cnn = cnnModel(TIME_PERIODS, n_sensors, n_classes)\n",
    "# Initialiser les poids avec Xavier initialization\n",
    "model_cnn.init_weights()\n",
    "# Move the model to the device (CPU or GPU)\n",
    "model_cnn.to(device)\n",
    "# Print model summary\n",
    "print(model_cnn)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "test_accs = []\n",
    "test_losses = []\n",
    "\n",
    "\n",
    "\n",
    "# Choose your optimizer\n",
    "#my_optimizer = torch.optim.SGD(model_cnn.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "my_optimizer = torch.optim.SGD(model_cnn.parameters(), lr=0.01, momentum=0.6)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(my_optimizer, T_max=100, eta_min=0)\n",
    "\n",
    "#lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "momentum_scheduler = torch.optim.lr_scheduler.ExponentialLR(my_optimizer, gamma=0.9)\n",
    "print('Training the model...')\n",
    "train_losses, val_losses, train_accs, val_accs = train_cnn(model_cnn, device, EPOCHS, my_optimizer)\n",
    "plot_performance(train_losses, val_losses, train_accs, val_accs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065ef272-1cec-442c-a8ba-b969579cad77",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf718d9-4a86-4f3a-9a57-73a9fcfe7291",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e295cdf-644a-461f-8606-38c0a00be705",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe478134-bbcc-4926-aaca-267320fb21fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
