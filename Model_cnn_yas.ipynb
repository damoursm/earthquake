{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abf2ed4-ac64-4491-a768-ed54ab5fe6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "from __future__ import print_function\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# PyTorch Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c35e784-6b0b-42b5-96dc-c21642b56a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 3, 12000)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import math\n",
    "\n",
    "def load_dataset(filename):\n",
    "    with h5py.File(filename, 'r') as f:\n",
    "        if 'data' in f:\n",
    "            data = f['data']\n",
    "            if isinstance(data, h5py.Dataset):\n",
    "                print(\"Type du dataset 'data':\", type(data))\n",
    "                print(\"Forme du dataset 'data':\", data.shape)\n",
    "                data_matrix = data[:]  \n",
    "                print(\"Matrice de données:\", data_matrix)\n",
    "                return data_matrix\n",
    "            elif isinstance(data, h5py.Group):\n",
    "                data_list = []\n",
    "                trace_names = []\n",
    "                for key in data.keys():\n",
    "                    dataset = data[key]\n",
    "                    if isinstance(dataset, h5py.Dataset):\n",
    "                        # Vérifier la forme des données extraites\n",
    "                        if dataset.ndim == 2:\n",
    "                            data_array = np.zeros((1, 3, dataset.shape[1]), dtype=np.float32)  # Créer une matrice vide de quatre dimensions\n",
    "                            for i in range(3):  # Boucle à travers les canaux\n",
    "                                data_array[:, i, :] = dataset[i, :].reshape(1,  dataset.shape[1])  # Réorganiser les données dans le bon ordre\n",
    "                            data_list.append(data_array)\n",
    "                            #trace_names.append(key)\n",
    "                            # Concaténer les noms de clés avec un tableau de zéros\n",
    "                            trace_names = np.concatenate([np.array([key] ).reshape(-1, 1) for key in data.keys()])\n",
    "                            # Remodeler en un vecteur colonne\n",
    "                            trace_names=trace_names.reshape(-1, 1)                          \n",
    "                            #trace_names['trace_names']=trace_names\n",
    "                        \n",
    "                        else:\n",
    "                            print(f\"Les données pour la clé '{key}' ne sont pas sous forme de tableau 2D.\")\n",
    "                if data_list:\n",
    "                    stacked_data_matrix = np.vstack(data_list)  # Empiler les matrices de données dans une seule matrice\n",
    "                    return stacked_data_matrix, trace_names\n",
    "                    \n",
    "# Utilisation de la foncti, our charger et convertir le dataset à partir du fichier HDF5\n",
    "\n",
    "\n",
    "filename1 = '20kEarthquakeAugmente.hdf5'\n",
    "noise_matrix, trace_names_noise = load_dataset(filename1)\n",
    "\n",
    "filename2 = '20kNoiseAugmente.hdf5'\n",
    "events_matrix, trace_names_events = load_dataset(filename2)\n",
    "\n",
    "print (events_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "976c11fa-3a14-4868-923a-5521b799b5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 3, 12000)\n",
      "(20000, 3, 12000)\n",
      "[['20050103T055721.MN.CUC..HH' '1']\n",
      " ['20050103T194108.IV.BRMO..EH' '1']\n",
      " ['20050103T194108.IV.CTI..EH' '1']\n",
      " ...\n",
      " ['20101003T082539.IV.BOB..HH' '1']\n",
      " ['20101003T082539.IV.POFI..HN' '1']\n",
      " ['20101003T095135.IV.RMP..HH' '1']]\n"
     ]
    }
   ],
   "source": [
    "# Création d'une colonne 'source_type' avec des valeurs constantes de 1\n",
    "source_type_n = np.zeros((len(trace_names_noise), 1), dtype=int)\n",
    "\n",
    "# Création d'une colonne 'source_type' avec des valeurs constantes de 1\n",
    "source_type_e = np.ones((len(trace_names_events), 1), dtype=int)\n",
    "\n",
    "# Concaténation de la colonne 'source_type' avec trace_names_noise\n",
    "trace_names_n = np.concatenate((trace_names_noise, source_type_n), axis=1)\n",
    "trace_names_e = np.concatenate((trace_names_events, source_type_e), axis=1)\n",
    "\n",
    "\n",
    "trace_names_e = np.concatenate((trace_names_events[:len(trace_names_noise)], source_type_e[:len(trace_names_noise)]), axis=1)\n",
    "events_matrix= events_matrix[:len(noise_matrix),:,:]\n",
    "print(events_matrix.shape)\n",
    "print(noise_matrix.shape)\n",
    "\n",
    "# Vérification de la forme de la matrice résultante\n",
    "print(trace_names_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd8b0b8-2ae4-4e3b-98a6-816447ef488c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25600,)\n",
      "(12800, 3, 12000)\n"
     ]
    }
   ],
   "source": [
    "# Préparation des données d'entrainement, validation et de test\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset\n",
    "import torch \n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "noise_matrix=events_matrix[0:10000, :, :]\n",
    "events_matrix=noise_matrix[0:10000, :, :]\n",
    "\n",
    "# Diviser les données de bruit en ensemble d'entraînement et ensemble de test\n",
    "df_noise_train1, df_noise_test = train_test_split(noise_matrix, test_size=0.20, random_state=42)\n",
    "df_noise_train, df_noise_validation = train_test_split(df_noise_train1, test_size=0.20, random_state=42)\n",
    "\n",
    "trace_noise1, trace_noise_test =train_test_split(trace_names_n, test_size=0.20, random_state=42)\n",
    "trace_noise_train, trace_noise_val = train_test_split(trace_noise1, test_size=0.20, random_state=42)\n",
    "\n",
    "trace_events1, trace_events_test =train_test_split(trace_names_e, test_size=0.20, random_state=42)\n",
    "trace_events_train, trace_events_val = train_test_split(trace_events1, test_size=0.20, random_state=42)\n",
    "\n",
    "\n",
    "df_train1, df_test = train_test_split(events_matrix, test_size=0.20, random_state=42)\n",
    "df_train, df_validation = train_test_split(df_train1, test_size=0.20, random_state=42)\n",
    "\n",
    "\n",
    "# Concaténer df_noise_train avec df_train\n",
    "df_train_concat = np.concatenate([df_train, df_noise_train], axis=0)\n",
    "trace_train_y = np.concatenate([trace_events_train, trace_noise_train], axis=0)\n",
    "trace_train = trace_train_y[:,1].astype(int)\n",
    "\n",
    "# Concaténer df_noise_test avec df_test\n",
    "df_test_concat = np.concatenate([df_test, df_noise_test], axis=0)\n",
    "trace_test_y= np.concatenate([trace_events_test, trace_noise_test], axis=0)\n",
    "# Convertir les étiquettes en entiers\n",
    "trace_test = trace_test_y[:,1].astype(int)\n",
    "\n",
    "#print (df_test_concat)\n",
    "# Concaténer df_noise_validation avec df_validation\n",
    "df_val_concat = np.concatenate([df_validation, df_noise_validation], axis=0)\n",
    "trace_val_y= np.concatenate([trace_events_val, trace_noise_val], axis=0)\n",
    "trace_val = trace_val_y[:,1].astype(int)\n",
    "\"\"\"\n",
    "# Normaliser toute la donnée\n",
    "mean_t = np.mean(df_train_concat)\n",
    "std_t = np.std(df_train_concat)\n",
    "norm_data_t = (df_train_concat - mean_t) / std_t\n",
    "\n",
    "#print(norm_data_t)\n",
    "\n",
    "mean_v = np.mean(df_val_concat)\n",
    "std_v = np.std(df_val_concat)\n",
    "norm_data_v = (df_val_concat - mean_v) / std_v\n",
    "\n",
    "mean_ts = np.mean(df_test_concat)\n",
    "std_ts = np.std(df_test_concat)\n",
    "norm_data_ts = (df_test_concat - mean_ts) / std_ts\n",
    "\"\"\"\n",
    "# Normaliser toute la donnée\n",
    "def normalize_data(data):\n",
    "    mean = np.mean(data, axis=(1, 2), keepdims=True)  # Calculer la moyenne de chaque échantillon\n",
    "    std = np.std(data, axis=(1, 2), keepdims=True)    # Calculer l'écart-type de chaque échantillon\n",
    "    norm_data = (data - mean) / std                  # Normaliser chaque échantillon\n",
    "    return norm_data\n",
    "\n",
    "# Appliquer la normalisation sur les données d'entraînement, de validation et de test\n",
    "norm_data_train = normalize_data(df_train_concat)\n",
    "norm_data_val = normalize_data(df_val_concat)\n",
    "norm_data_test = normalize_data(df_test_concat)\n",
    "norm_data_t = df_train_concat \n",
    "norm_data_v = df_val_concat \n",
    "norm_data_ts = df_test_concat              \n",
    "\n",
    "print(trace_train.shape)\n",
    "\n",
    "print(norm_data_t.shape)\n",
    "\n",
    "# Convert your numpy arrays to PyTorch tensors\n",
    "x_train_tensor = torch.tensor(norm_data_t, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(trace_train, dtype=torch.long)  # long for CrossEntropyLoss\n",
    "\n",
    "x_test_tensor = torch.tensor(norm_data_ts, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(trace_test, dtype=torch.long)\n",
    "\n",
    "x_val_tensor = torch.tensor(norm_data_v, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(trace_val, dtype=torch.long)\n",
    "\n",
    "print(x_val_tensor)\n",
    "print( y_val_tensor)\n",
    "\n",
    "y_train_tensor = y_train_tensor.unsqueeze(1)\n",
    "y_val_tensor = y_val_tensor.unsqueeze(1)\n",
    "y_test_tensor = y_test_tensor.unsqueeze(1)\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
    "val_dataset = TensorDataset(x_val_tensor, y_val_tensor)\n",
    "\n",
    "\n",
    "# Créer les DataLoaders pour l'entraînement, la validation et le test\n",
    "batch_size =64  # Vous pouvez changer cette valeur selon vos besoins\n",
    "\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71e7c56e-41ea-48ea-aa19-2171e2ffbf00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape (Reshape)           (None, 12000, 3)          0         \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 11991, 100)        3100      \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 11982, 100)        100100    \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1  (None, 3994, 100)         0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 3985, 160)         160160    \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 3976, 160)         256160    \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 160)               0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 160)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 161       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 519681 (1.98 MB)\n",
      "Trainable params: 519681 (1.98 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'norm_data_v' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 43\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Compile the model\u001b[39;00m\n\u001b[0;32m     41\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 43\u001b[0m data_v_transpo\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtranspose(norm_data_v, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     44\u001b[0m data_tr_transpo\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtranspose(norm_data_t, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'norm_data_v' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "########################################   Model1 CNN   ###########################\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix, classification_report\n",
    "\n",
    "# Define the input shape\n",
    "input_shape = (3, 12000)\n",
    "\n",
    "# Create a deeper CNN model\n",
    "\n",
    "from tensorflow.keras import models, layers\n",
    "\n",
    "# Define the input shape\n",
    "input_shape = (3, 12000)\n",
    "EPOCHS=50\n",
    "TIME_PERIODS=12000\n",
    "n_sensors=3\n",
    "n_classes=2\n",
    "BATCH =64\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Reshape((TIME_PERIODS, n_sensors), input_shape=(TIME_PERIODS, n_sensors)),\n",
    "    layers.Conv1D(100, 10, activation='relu', input_shape=(TIME_PERIODS, n_sensors)),\n",
    "    layers.Conv1D(100, 10, activation='relu'),\n",
    "    layers.MaxPooling1D(3),\n",
    "    layers.Conv1D(160, 10, activation='relu'),\n",
    "    layers.Conv1D(160, 10, activation='relu'),\n",
    "    layers.GlobalAveragePooling1D(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "data_v_transpo= np.transpose(norm_data_v, (0, 2, 1))\n",
    "data_tr_transpo= np.transpose(norm_data_t, (0, 2, 1))\n",
    "\n",
    "\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(data_tr_transpo, trace_train, epochs=EPOCHS, batch_size=BATCH, validation_data=(data_v_transpo, trace_val))\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "data_ts_transpo= np.transpose(norm_data_ts, (0, 2, 1))\n",
    "\n",
    "# Make predictions on test data\n",
    "predictions = model.predict(data_ts_transpo)\n",
    "binary_predictions = (predictions > 0.5).astype(int)\n",
    "predictions = model.predict(data_v_transpo)\n",
    "data_ts_transpo\n",
    "\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(data_ts_transpo, trace_test)\n",
    "print('Test accuracy:', test_acc)\n",
    "\n",
    "# Make predictions on test data\n",
    "predictions = model.predict(data_ts_transpo)\n",
    "binary_predictions = (predictions > 0.5).astype(int)\n",
    "\n",
    "#Obtenir les prédictions sur l'ensemble de validation\n",
    "predictions_v = model.predict(data_v_transpo)\n",
    "binary_predictions_v = (predictions_v > 0.5).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "# Calculate ROC curve and AUC\n",
    "fpr, tpr, thresholds = roc_curve(trace_test, predictions)\n",
    "roc_auc = roc_auc_score(trace_test, predictions)\n",
    "\n",
    " \n",
    "# Plot ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(trace_test, binary_predictions)\n",
    "\n",
    "trace_val_y_array = trace_val_y\n",
    "\n",
    "# Convertir les tenseurs en tableaux NumPy\n",
    "trace_val_y_array_np = trace_val_y_array\n",
    "\n",
    "# Concaténer les tableaux NumPy le long de l'axe 1\n",
    "concatenated_array_v = np.concatenate((trace_val_y_array_np, binary_predictions_v), axis=1)\n",
    "\n",
    "# Convertir les tenseurs TensorFlow en tableaux NumPy\n",
    "trace_test_y_array = trace_test_y\n",
    "\n",
    "# Convertir les tenseurs en tableaux NumPy\n",
    "trace_test_y_array_np = trace_test_y_array\n",
    "# Concaténer les tableaux NumPy côte à côte\n",
    "concatenated_array_ts =np.concatenate((trace_test_y_array_np, binary_predictions), axis=1)  \n",
    "concatenated_array = np.concatenate([concatenated_array_ts, concatenated_array_v], axis=0)\n",
    "# Convertir le tableau NumPy concaténé en DataFrame pandas\n",
    "result_df = pd.DataFrame(concatenated_array, columns=['trace_name', 'source_type','binary_predictions'])\n",
    "\n",
    "# Sauvegarder le résultat dans un fichier CSV\n",
    "result_df.to_csv('result.csv', index=False)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.xticks([0, 1])\n",
    "plt.yticks([0, 1])\n",
    "plt.tight_layout()\n",
    "\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    for j in range(conf_matrix.shape[1]):\n",
    "        plt.text(j, i, format(conf_matrix[i, j], 'd'), horizontalalignment=\"center\", color=\"white\" if conf_matrix[i, j] > conf_matrix.max() / 2 else \"black\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Print classification report\n",
    "class_report = classification_report(trace_test, binary_predictions)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfd1996-90ad-4ee9-969d-5062c5dba951",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###########################   Model2 CNN  ##################################################\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix, classification_report\n",
    "\n",
    "# Define the input shape\n",
    "input_shape = (3, 12000)\n",
    "EPOCHS=50\n",
    "TIME_PERIODS=12000\n",
    "n_sensors=3\n",
    "n_classes=2\n",
    "BATCH =64\n",
    "\n",
    "# Define the input shape\n",
    "input_shape = (3, 12000)\n",
    "\n",
    "# Create a simple CNN model\n",
    "model = models.Sequential([\n",
    "    layers.Conv1D(32, 3, activation='relu', input_shape=input_shape),\n",
    "    layers.MaxPooling1D(1),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(norm_data_t, trace_train, epochs=40, batch_size=BATCH, validation_data=(norm_data_v, trace_val))\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(norm_data_ts, trace_test)\n",
    "print('Test accuracy:', test_acc)\n",
    "\n",
    "# Make predictions on test data\n",
    "predictions = model.predict(norm_data_ts)\n",
    "binary_predictions = (predictions > 0.5).astype(int)\n",
    "\n",
    "# Calculate ROC curve and AUC\n",
    "fpr, tpr, thresholds = roc_curve(trace_test, predictions)\n",
    "roc_auc = roc_auc_score(trace_test, predictions)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(trace_test, binary_predictions)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.xticks([0, 1])\n",
    "plt.yticks([0, 1])\n",
    "plt.tight_layout()\n",
    "\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    for j in range(conf_matrix.shape[1]):\n",
    "        plt.text(j, i, format(conf_matrix[i, j], 'd'), horizontalalignment=\"center\", color=\"white\" if conf_matrix[i, j] > conf_matrix.max() / 2 else \"black\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Print classification report\n",
    "class_report = classification_report(trace_test, binary_predictions)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cdc6a0-f8c5-429c-9ca7-42f635c855c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93593d39-a21d-4777-9ccc-27cf5dbc95df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a643835c-17d3-448c-ab8a-a85efd49605e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce173ee8-0ed8-4f66-a4ed-5ff583963053",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f163ebe8-e56f-4112-8690-b03ce67620b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c726b770-77a5-48c9-a69b-42c51c68f2a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e73a524-e149-4571-8101-2e8dab1a4e47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7579b83-a1ab-4d41-957b-6f151f2b24ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e3618e-a6c8-46ab-893e-e5499f13a9b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
